# Projeto Big Data - AnÃ¡lise de Mercados Financeiros

## ğŸ“Š VisÃ£o Geral

Este projeto implementa uma plataforma completa de anÃ¡lise de Big Data para mercados financeiros, integrando coleta de dados em tempo real, processamento distribuÃ­do, anÃ¡lise de sentimentos e machine learning para identificar correlaÃ§Ãµes entre eventos mundiais e movimentos de mercado.

### ğŸ¯ Objetivos

- **Coleta Automatizada**: Dados financeiros (Yahoo Finance) e eventos mundiais (APIs de notÃ­cias)
- **Processamento DistribuÃ­do**: Apache Spark e HDFS para anÃ¡lise de grandes volumes
- **AnÃ¡lise AvanÃ§ada**: CorrelaÃ§Ãµes entre eventos e movimentos de mercado
- **Machine Learning**: Modelos preditivos e anÃ¡lise de sentimentos
- **VisualizaÃ§Ã£o**: Dashboards interativos e relatÃ³rios automatizados

## ğŸ‘¥ Equipe de Desenvolvimento

| ResponsÃ¡vel | Ãrea de AtuaÃ§Ã£o | MÃ³dulos |
|-------------|-----------------|---------|
| **Fabio** | GestÃ£o do Projeto | CoordenaÃ§Ã£o geral, planejamento |
| **Felipe Martins** | APIs e Coleta de Dados | Yahoo Finance, News API, endpoints |
| **Ana Luiza Pazze** | Arquitetura e Infraestrutura | Spark, HDFS, Docker, configuraÃ§Ãµes |
| **Pedro Silva** | AnÃ¡lise EstatÃ­stica | AnÃ¡lise exploratÃ³ria, estatÃ­sticas |
| **Anny Caroline Sousa** | Machine Learning | Modelos preditivos, sentiment analysis |
| **Ricardo Areas** | VisualizaÃ§Ã£o | Dashboards, grÃ¡ficos interativos |

## ğŸ—ï¸ Arquitetura do Sistema

```
ProjetoBigData/
â”œâ”€â”€ src/                          # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ data_collection/          # Coleta de dados
â”‚   â”‚   â”œâ”€â”€ yahoo_finance_collector.py
â”‚   â”‚   â””â”€â”€ news_collector.py
â”‚   â”œâ”€â”€ infrastructure/           # Infraestrutura distribuÃ­da
â”‚   â”‚   â”œâ”€â”€ spark_manager.py
â”‚   â”‚   â””â”€â”€ hdfs_manager.py
â”‚   â”œâ”€â”€ data_analysis/           # AnÃ¡lise estatÃ­stica
â”‚   â”‚   â”œâ”€â”€ statistical_analyzer.py
â”‚   â”‚   â””â”€â”€ exploratory_analyzer.py
â”‚   â”œâ”€â”€ machine_learning/        # ML e Sentiment Analysis
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.py
â”‚   â”‚   â””â”€â”€ predictive_models.py
â”‚   â””â”€â”€ visualization/           # Dashboards e grÃ¡ficos
â”‚       â”œâ”€â”€ dashboard.py
â”‚       â””â”€â”€ charts.py
â”œâ”€â”€ data/                        # Dados do projeto
â”‚   â”œâ”€â”€ raw/                     # Dados brutos
â”‚   â”œâ”€â”€ processed/               # Dados processados
â”‚   â””â”€â”€ external/                # Dados externos
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”œâ”€â”€ tests/                       # Testes unitÃ¡rios
â”œâ”€â”€ docker/                      # ConfiguraÃ§Ãµes Docker
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ spark/
â”œâ”€â”€ config/                      # ConfiguraÃ§Ãµes
â””â”€â”€ docs/                        # DocumentaÃ§Ã£o
```

## ğŸš€ Tecnologias Utilizadas

### Core Technologies
- **Python 3.9+**: Linguagem principal
- **Apache Spark 3.4.0**: Processamento distribuÃ­do
- **Hadoop HDFS**: Armazenamento distribuÃ­do
- **Docker & Docker Compose**: ContainerizaÃ§Ã£o

### Data & Analytics
- **pandas**: ManipulaÃ§Ã£o de dados
- **numpy**: ComputaÃ§Ã£o numÃ©rica
- **scikit-learn**: Machine learning
- **scipy**: AnÃ¡lise estatÃ­stica
- **Delta Lake**: Versionamento de dados

### APIs & Data Sources
- **yfinance**: Yahoo Finance API
- **newsapi-python**: News API
- **requests**: HTTP requests
- **beautifulsoup4**: Web scraping

### Visualization
- **streamlit**: Dashboards web
- **plotly**: GrÃ¡ficos interativos
- **matplotlib**: VisualizaÃ§Ãµes estÃ¡ticas
- **seaborn**: VisualizaÃ§Ãµes estatÃ­sticas

### NLP & Sentiment Analysis
- **textblob**: AnÃ¡lise de sentimentos
- **vaderSentiment**: Sentiment analysis
- **nltk**: Processamento de linguagem natural

### Infrastructure & Monitoring
- **jupyter**: Notebooks interativos
- **pytest**: Testes unitÃ¡rios
- **python-dotenv**: Gerenciamento de configuraÃ§Ãµes

## ğŸ“‹ PrÃ©-requisitos

### Sistema
- **Docker**: 28.4.0+
- **Docker Compose**: 2.0+
- **Python**: 3.9+
- **Git**: Para controle de versÃ£o

### APIs (Opcionais)
- **News API Key**: Para coleta de notÃ­cias
- **Alpha Vantage API**: Dados financeiros alternativos

## ğŸ› ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clone o RepositÃ³rio
```bash
git clone <repository-url>
cd ProjetoBigData
```

### 2. ConfiguraÃ§Ã£o de Ambiente

#### OpÃ§Ã£o A: Docker (Recomendado)
```bash
# Construir e iniciar todos os serviÃ§os
docker-compose up -d

# Verificar status dos containers
docker-compose ps

# Acessar logs
docker-compose logs -f
```

#### OpÃ§Ã£o B: InstalaÃ§Ã£o Local
```bash
# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 3. ConfiguraÃ§Ã£o de APIs
```bash
# Copiar arquivo de configuraÃ§Ã£o
cp config/.env.example config/.env

# Editar com suas chaves de API
nano config/.env
```

Exemplo de `.env`:
```env
# APIs
NEWS_API_KEY=your_news_api_key_here
ALPHA_VANTAGE_KEY=your_alpha_vantage_key_here

# Spark Configuration
SPARK_MASTER_URL=spark://spark-master:7077
HDFS_NAMENODE_URL=hdfs://namenode:9000

# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=bigdata_finance
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
```

## ğŸ® Como Usar

### 1. Acesso aos ServiÃ§os

| ServiÃ§o | URL | DescriÃ§Ã£o |
|---------|-----|-----------|
| **Jupyter Notebook** | http://localhost:8888 | Notebooks interativos |
| **Spark Master UI** | http://localhost:8080 | Interface do Spark |
| **HDFS NameNode** | http://localhost:9870 | Interface do HDFS |
| **Streamlit Dashboard** | http://localhost:8501 | Dashboard principal |

### 2. Coleta de Dados

#### Yahoo Finance
```python
from src.data_collection.yahoo_finance_collector import YahooFinanceCollector

# Inicializar coletor
collector = YahooFinanceCollector()

# Coletar dados de aÃ§Ãµes
stock_data = collector.collect_stock_data(['AAPL', 'GOOGL', 'MSFT'])

# Coletar dados de Ã­ndices
index_data = collector.collect_index_data(['^GSPC', '^DJI', '^IXIC'])
```

#### NotÃ­cias e Eventos
```python
from src.data_collection.news_collector import NewsCollector

# Inicializar coletor de notÃ­cias
news_collector = NewsCollector()

# Coletar eventos histÃ³ricos
events = news_collector.collect_historical_events(
    start_date='2020-01-01',
    end_date='2023-12-31'
)
```

### 3. Processamento com Spark

```python
from src.infrastructure.spark_manager import SparkManager

# Inicializar Spark
spark_manager = SparkManager()

# Processar dados financeiros
processed_data = spark_manager.process_financial_data(
    input_path="hdfs://namenode:9000/raw/financial_data",
    output_path="hdfs://namenode:9000/processed/financial_data"
)

# Executar pipeline ETL completo
spark_manager.run_etl_pipeline()
```

### 4. AnÃ¡lise EstatÃ­stica

```python
from src.data_analysis.statistical_analyzer import StatisticalAnalyzer

# Inicializar analisador
analyzer = StatisticalAnalyzer()

# AnÃ¡lise descritiva
stats = analyzer.descriptive_statistics(data)

# Teste de normalidade
normality_results = analyzer.test_normality(data['returns'])

# AnÃ¡lise de correlaÃ§Ã£o
correlation_matrix = analyzer.correlation_analysis(data)
```

### 5. Machine Learning

```python
from src.machine_learning.predictive_models import PredictiveModels
from src.machine_learning.sentiment_analyzer import SentimentAnalyzer

# Modelos preditivos
ml_models = PredictiveModels()
model_results = ml_models.train_regression_models(data)

# AnÃ¡lise de sentimentos
sentiment_analyzer = SentimentAnalyzer()
sentiment_scores = sentiment_analyzer.analyze_sentiment_batch(news_texts)
```

### 6. VisualizaÃ§Ã£o

```python
from src.visualization.dashboard import FinancialDashboard
from src.visualization.charts import FinancialCharts

# Dashboard interativo
dashboard = FinancialDashboard()
dashboard.create_streamlit_dashboard()

# GrÃ¡ficos especializados
charts = FinancialCharts()
price_chart = charts.plot_price_action(data)
technical_chart = charts.plot_technical_indicators(data)
```

## ğŸ“Š Exemplos de Uso

### AnÃ¡lise de Impacto de Eventos
```python
# 1. Coletar dados financeiros
financial_data = collector.collect_stock_data(['SPY'], 
                                            start_date='2020-01-01',
                                            end_date='2023-12-31')

# 2. Coletar eventos
events = news_collector.collect_historical_events(
    keywords=['COVID', 'Federal Reserve', 'inflation'],
    start_date='2020-01-01',
    end_date='2023-12-31'
)

# 3. AnÃ¡lise de correlaÃ§Ã£o
correlation_results = analyzer.analyze_event_impact(
    financial_data, events, window_days=30
)

# 4. VisualizaÃ§Ã£o
impact_chart = charts.plot_event_impact(
    financial_data, events, title="Impacto de Eventos no S&P 500"
)
```

### Pipeline Completo de ML
```python
# 1. PreparaÃ§Ã£o de features
feature_engineer = FeatureEngineer()
features = feature_engineer.create_technical_indicators(data)
features = feature_engineer.create_lag_features(features)

# 2. Treinamento de modelos
models = PredictiveModels()
results = models.train_all_models(features, target='price_direction')

# 3. AvaliaÃ§Ã£o
evaluation = models.evaluate_models(results)

# 4. PrediÃ§Ãµes
predictions = models.predict(new_data)
```

## ğŸ§ª Testes

### Executar Testes
```bash
# Todos os testes
pytest tests/

# Testes especÃ­ficos
pytest tests/test_data_collection.py
pytest tests/test_spark_manager.py

# Com cobertura
pytest --cov=src tests/
```

### Estrutura de Testes
```
tests/
â”œâ”€â”€ test_data_collection/
â”‚   â”œâ”€â”€ test_yahoo_finance_collector.py
â”‚   â””â”€â”€ test_news_collector.py
â”œâ”€â”€ test_infrastructure/
â”‚   â”œâ”€â”€ test_spark_manager.py
â”‚   â””â”€â”€ test_hdfs_manager.py
â”œâ”€â”€ test_analysis/
â”‚   â”œâ”€â”€ test_statistical_analyzer.py
â”‚   â””â”€â”€ test_exploratory_analyzer.py
â””â”€â”€ test_ml/
    â”œâ”€â”€ test_sentiment_analyzer.py
    â””â”€â”€ test_predictive_models.py
```

## ğŸ“ˆ Monitoramento e Performance

### MÃ©tricas do Sistema
- **Spark Jobs**: Monitoramento via Spark UI
- **HDFS Usage**: UtilizaÃ§Ã£o de armazenamento
- **Memory Usage**: Consumo de memÃ³ria
- **Processing Time**: Tempo de processamento

### Logs
```bash
# Logs do Docker Compose
docker-compose logs -f

# Logs especÃ­ficos
docker-compose logs spark-master
docker-compose logs namenode
docker-compose logs jupyter
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Spark Configuration
```python
# spark-defaults.conf
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
```

### HDFS Configuration
```xml
<!-- core-site.xml -->
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:9000</value>
    </property>
</configuration>
```

## ğŸš¨ Troubleshooting

### Problemas Comuns

#### 1. Container nÃ£o inicia
```bash
# Verificar logs
docker-compose logs container_name

# Recriar containers
docker-compose down
docker-compose up -d --force-recreate
```

#### 2. Erro de memÃ³ria no Spark
```bash
# Ajustar configuraÃ§Ãµes no docker-compose.yml
environment:
  - SPARK_WORKER_MEMORY=4g
  - SPARK_DRIVER_MEMORY=2g
```

#### 3. HDFS nÃ£o acessÃ­vel
```bash
# Verificar status do namenode
docker-compose exec namenode hdfs dfsadmin -report

# Formatar namenode (CUIDADO: apaga dados)
docker-compose exec namenode hdfs namenode -format
```

#### 4. Jupyter nÃ£o carrega notebooks
```bash
# Verificar permissÃµes
docker-compose exec jupyter ls -la /home/jovyan/work/

# Recriar container
docker-compose restart jupyter
```

## ğŸ“š DocumentaÃ§Ã£o Adicional

### Notebooks de Exemplo
- `notebooks/01_data_collection_example.ipynb`: Coleta de dados
- `notebooks/02_spark_processing_example.ipynb`: Processamento Spark
- `notebooks/03_statistical_analysis_example.ipynb`: AnÃ¡lise estatÃ­stica
- `notebooks/04_ml_models_example.ipynb`: Machine Learning
- `notebooks/05_visualization_example.ipynb`: VisualizaÃ§Ãµes

### APIs Documentation
- [Yahoo Finance API](https://pypi.org/project/yfinance/)
- [News API](https://newsapi.org/docs)
- [Apache Spark](https://spark.apache.org/docs/latest/)
- [Streamlit](https://docs.streamlit.io/)

## ğŸ¤ ContribuiÃ§Ã£o

### Guidelines
1. Fork o repositÃ³rio
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### Code Style
- **PEP 8**: Seguir padrÃµes Python
- **Type Hints**: Usar anotaÃ§Ãµes de tipo
- **Docstrings**: Documentar funÃ§Ãµes e classes
- **Tests**: Incluir testes para novas funcionalidades

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ“ Suporte

### Contatos da Equipe
- **Felipe Martins**: felipe.martins@email.com (CoordenaÃ§Ã£o)
- **Pedro Silva**: pedro.silva@email.com (AnÃ¡lise)
- **Anny Caroline**: anny.sousa@email.com (ML)
- **Ricardo Areas**: ricardo.areas@email.com (VisualizaÃ§Ã£o)
- **Fabio**: fabio@email.com (DocumentaÃ§Ã£o)

### Issues e Bugs
- Reporte bugs via [GitHub Issues](https://github.com/your-repo/issues)
- Para dÃºvidas, use [GitHub Discussions](https://github.com/your-repo/discussions)

---

## ğŸ¯ Roadmap

### VersÃ£o 1.0 (Atual)
- âœ… Coleta de dados Yahoo Finance
- âœ… Processamento Spark/HDFS
- âœ… AnÃ¡lise estatÃ­stica bÃ¡sica
- âœ… Modelos de ML
- âœ… Dashboard Streamlit

### VersÃ£o 1.1 (PrÃ³xima)
- ğŸ”„ API REST para dados
- ğŸ”„ Alertas em tempo real
- ğŸ”„ Mais fontes de dados
- ğŸ”„ Modelos deep learning

### VersÃ£o 2.0 (Futuro)
- ğŸ“‹ Streaming de dados
- ğŸ“‹ Kubernetes deployment
- ğŸ“‹ Interface web completa
- ğŸ“‹ Mobile app

---

**Desenvolvido com â¤ï¸ pela Equipe Big Data Finance**