{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Machine Learning e AnÃ¡lise de Sentimentos - Big Data Finance\n",
    "\n",
    "Este notebook demonstra como aplicar tÃ©cnicas de Machine Learning e anÃ¡lise de sentimentos aos dados financeiros.\n",
    "\n",
    "## Objetivos\n",
    "- Engenharia de features para dados financeiros\n",
    "- Modelos preditivos para preÃ§os e direÃ§Ã£o\n",
    "- AnÃ¡lise de sentimentos de notÃ­cias\n",
    "- CorrelaÃ§Ã£o entre sentimentos e preÃ§os\n",
    "- AvaliaÃ§Ã£o e otimizaÃ§Ã£o de modelos\n",
    "- Interpretabilidade dos resultados\n",
    "\n",
    "**Autor:** Anny Caroline Sousa (Machine Learning) & Equipe Big Data Finance  \n",
    "**GestÃ£o:** Fabio  \n",
    "**Infraestrutura:** Ana Luiza Pazze  \n",
    "**Data:** 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessÃ¡rios\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Imports de ML\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Imports dos mÃ³dulos do projeto\n",
    "from machine_learning.sentiment_analyzer import SentimentAnalyzer\n",
    "from machine_learning.predictive_models import PredictiveModels, FeatureEngineer\n",
    "from data_analysis.statistical_analyzer import StatisticalAnalyzer\n",
    "\n",
    "print(\"âœ… Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸ“Š Carregamento e PreparaÃ§Ã£o dos Dados\n",
    "\n",
    "Vamos carregar os dados processados pelo Spark e preparar para ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar dados processados disponÃ­veis\n",
    "processed_dir = '../data/processed'\n",
    "raw_dir = '../data/raw'\n",
    "\n",
    "# Tentar carregar dados do Spark primeiro\n",
    "if os.path.exists(f\"{processed_dir}/stock_indicators_csv\"):\n",
    "    print(\"ðŸ“Š Carregando dados processados pelo Spark...\")\n",
    "    # Encontrar arquivo CSV dentro do diretÃ³rio\n",
    "    csv_files = [f for f in os.listdir(f\"{processed_dir}/stock_indicators_csv\") if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        df = pd.read_csv(f\"{processed_dir}/stock_indicators_csv/{csv_files[0]}\")\n",
    "        print(f\"âœ… Dados carregados: {len(df)} registros\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Arquivo CSV nÃ£o encontrado no diretÃ³rio processado\")\n",
    "else:\n",
    "    print(\"âš ï¸ Dados processados nÃ£o encontrados. Carregando dados brutos...\")\n",
    "    # Carregar dados brutos\n",
    "    stock_files = [f for f in os.listdir(raw_dir) if f.startswith('stock_data_') and f.endswith('.csv')]\n",
    "    if stock_files:\n",
    "        latest_file = sorted(stock_files)[-1]\n",
    "        df = pd.read_csv(f\"{raw_dir}/{latest_file}\")\n",
    "        print(f\"ðŸ“ Carregando dados brutos: {latest_file}\")\n",
    "        print(f\"âœ… Dados carregados: {len(df)} registros\")\n",
    "    else:\n",
    "        print(\"ðŸ”§ Criando dados sintÃ©ticos para demonstraÃ§Ã£o...\")\n",
    "        # Criar dados sintÃ©ticos\n",
    "        dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='D')\n",
    "        symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "        \n",
    "        data = []\n",
    "        for symbol in symbols:\n",
    "            base_price = np.random.uniform(100, 300)\n",
    "            for i, date in enumerate(dates):\n",
    "                price_change = np.random.normal(0, 0.02)\n",
    "                base_price *= (1 + price_change)\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date.strftime('%Y-%m-%d'),\n",
    "                    'symbol': symbol,\n",
    "                    'open': base_price * np.random.uniform(0.98, 1.02),\n",
    "                    'high': base_price * np.random.uniform(1.00, 1.05),\n",
    "                    'low': base_price * np.random.uniform(0.95, 1.00),\n",
    "                    'close': base_price,\n",
    "                    'volume': np.random.randint(1000000, 10000000)\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"âœ… Dados sintÃ©ticos criados: {len(df)} registros\")\n",
    "\n",
    "# Converter data\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"\\nðŸ“‹ Colunas disponÃ­veis: {list(df.columns)}\")\n",
    "print(f\"ðŸ“… PerÃ­odo: {df['date'].min()} atÃ© {df['date'].max()}\")\n",
    "print(f\"ðŸ“ˆ SÃ­mbolos: {df['symbol'].unique()}\")\n",
    "\n",
    "# Visualizar primeiros registros\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸ”§ Engenharia de Features\n",
    "\n",
    "Vamos criar features tÃ©cnicas para os modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Feature Engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "print(\"ðŸ”§ Criando features tÃ©cnicas...\")\n",
    "\n",
    "# Processar cada sÃ­mbolo separadamente\n",
    "enhanced_data = []\n",
    "\n",
    "for symbol in df['symbol'].unique():\n",
    "    print(f\"ðŸ“Š Processando {symbol}...\")\n",
    "    \n",
    "    # Filtrar dados do sÃ­mbolo\n",
    "    symbol_data = df[df['symbol'] == symbol].copy().sort_values('date')\n",
    "    \n",
    "    # Criar indicadores tÃ©cnicos\n",
    "    symbol_data = feature_engineer.create_technical_indicators(symbol_data)\n",
    "    \n",
    "    # Criar features de lag\n",
    "    symbol_data = feature_engineer.create_lag_features(symbol_data, ['close', 'volume'], lags=[1, 2, 3, 5])\n",
    "    \n",
    "    # Criar features de rolling\n",
    "    symbol_data = feature_engineer.create_rolling_features(symbol_data, ['close', 'volume'], windows=[5, 10, 20])\n",
    "    \n",
    "    enhanced_data.append(symbol_data)\n",
    "\n",
    "# Combinar todos os dados\n",
    "df_enhanced = pd.concat(enhanced_data, ignore_index=True)\n",
    "\n",
    "# Remover NaN (devido aos lags e rolling)\n",
    "df_enhanced = df_enhanced.dropna()\n",
    "\n",
    "print(f\"âœ… Features criadas: {len(df_enhanced.columns)} colunas\")\n",
    "print(f\"ðŸ“Š Registros apÃ³s limpeza: {len(df_enhanced)}\")\n",
    "\n",
    "# Mostrar novas features\n",
    "new_features = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "print(f\"\\nðŸ†• Novas features ({len(new_features)}):\")\n",
    "for i, feature in enumerate(new_features[:15]):  # Mostrar apenas as primeiras 15\n",
    "    print(f\"  {i+1:2d}. {feature}\")\n",
    "if len(new_features) > 15:\n",
    "    print(f\"  ... e mais {len(new_features) - 15} features\")\n",
    "\n",
    "display(df_enhanced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¯ PreparaÃ§Ã£o dos Targets\n",
    "\n",
    "Vamos criar diferentes targets para nossos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Predictive Models\n",
    "predictive_models = PredictiveModels()\n",
    "\n",
    "print(\"ðŸŽ¯ Criando targets para prediÃ§Ã£o...\")\n",
    "\n",
    "# Preparar dados para cada sÃ­mbolo\n",
    "model_data = {}\n",
    "\n",
    "for symbol in df_enhanced['symbol'].unique():\n",
    "    print(f\"\\nðŸ“Š Preparando dados para {symbol}...\")\n",
    "    \n",
    "    # Filtrar dados do sÃ­mbolo\n",
    "    symbol_data = df_enhanced[df_enhanced['symbol'] == symbol].copy().sort_values('date')\n",
    "    \n",
    "    # Preparar dados\n",
    "    X, y_price, y_direction, y_volatility = predictive_models.prepare_data(\n",
    "        symbol_data, \n",
    "        target_days=1\n",
    "    )\n",
    "    \n",
    "    model_data[symbol] = {\n",
    "        'X': X,\n",
    "        'y_price': y_price,\n",
    "        'y_direction': y_direction,\n",
    "        'y_volatility': y_volatility,\n",
    "        'dates': symbol_data['date'].iloc[:-1].values  # Excluir Ãºltima data (sem target)\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ… Features: {X.shape[1]} | Amostras: {X.shape[0]}\")\n",
    "    print(f\"  ðŸ“ˆ DistribuiÃ§Ã£o direÃ§Ã£o: {np.bincount(y_direction)}\")\n",
    "\n",
    "# Combinar dados de todos os sÃ­mbolos\n",
    "X_combined = np.vstack([data['X'] for data in model_data.values()])\n",
    "y_price_combined = np.hstack([data['y_price'] for data in model_data.values()])\n",
    "y_direction_combined = np.hstack([data['y_direction'] for data in model_data.values()])\n",
    "y_volatility_combined = np.hstack([data['y_volatility'] for data in model_data.values()])\n",
    "\n",
    "print(f\"\\nðŸ“Š DADOS COMBINADOS:\")\n",
    "print(f\"  ðŸ“ˆ Total de amostras: {X_combined.shape[0]:,}\")\n",
    "print(f\"  ðŸ”§ Total de features: {X_combined.shape[1]}\")\n",
    "print(f\"  ðŸ“Š DistribuiÃ§Ã£o direÃ§Ã£o geral: {np.bincount(y_direction_combined)}\")\n",
    "\n",
    "# Nomes das features\n",
    "feature_names = [col for col in df_enhanced.columns if col not in ['date', 'symbol']]\n",
    "print(f\"\\nðŸ“‹ Features principais:\")\n",
    "for i, name in enumerate(feature_names[:10]):\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "if len(feature_names) > 10:\n",
    "    print(f\"  ... e mais {len(feature_names) - 10} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸ¤– Modelos de RegressÃ£o (PrediÃ§Ã£o de PreÃ§os)\n",
    "\n",
    "Vamos treinar modelos para prever preÃ§os futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dados para treino e teste\n",
    "print(\"ðŸ”„ Dividindo dados para treino e teste...\")\n",
    "\n",
    "# Usar 80% para treino, 20% para teste (mantendo ordem temporal)\n",
    "split_idx = int(0.8 * len(X_combined))\n",
    "\n",
    "X_train, X_test = X_combined[:split_idx], X_combined[split_idx:]\n",
    "y_price_train, y_price_test = y_price_combined[:split_idx], y_price_combined[split_idx:]\n",
    "y_dir_train, y_dir_test = y_direction_combined[:split_idx], y_direction_combined[split_idx:]\n",
    "\n",
    "print(f\"ðŸ“Š Treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"ðŸ“Š Teste: {X_test.shape[0]:,} amostras\")\n",
    "\n",
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Dados normalizados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos de regressÃ£o\n",
    "print(\"ðŸ¤– Treinando modelos de regressÃ£o...\")\n",
    "\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\nðŸ”§ Treinando {name}...\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model.fit(X_train_scaled, y_price_train)\n",
    "    \n",
    "    # Fazer prediÃ§Ãµes\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular mÃ©tricas\n",
    "    train_mse = mean_squared_error(y_price_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_price_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_price_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_price_test, y_pred_test)\n",
    "    \n",
    "    regression_results[name] = {\n",
    "        'model': model,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  ðŸ“Š Train RÂ²: {train_r2:.4f} | Test RÂ²: {test_r2:.4f}\")\n",
    "    print(f\"  ðŸ“Š Train MSE: {train_mse:.4f} | Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Todos os modelos de regressÃ£o treinados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar performance dos modelos\n",
    "print(\"ðŸ“Š COMPARAÃ‡ÃƒO DE PERFORMANCE - REGRESSÃƒO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': list(regression_results.keys()),\n",
    "    'Train RÂ²': [results['train_r2'] for results in regression_results.values()],\n",
    "    'Test RÂ²': [results['test_r2'] for results in regression_results.values()],\n",
    "    'Train MSE': [results['train_mse'] for results in regression_results.values()],\n",
    "    'Test MSE': [results['test_mse'] for results in regression_results.values()]\n",
    "})\n",
    "\n",
    "# Ordenar por Test RÂ²\n",
    "results_df = results_df.sort_values('Test RÂ²', ascending=False)\n",
    "display(results_df)\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# RÂ² Score\n",
    "x_pos = np.arange(len(results_df))\n",
    "axes[0].bar(x_pos - 0.2, results_df['Train RÂ²'], 0.4, label='Train', alpha=0.7)\n",
    "axes[0].bar(x_pos + 0.2, results_df['Test RÂ²'], 0.4, label='Test', alpha=0.7)\n",
    "axes[0].set_xlabel('Modelos')\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].set_title('ðŸ“Š ComparaÃ§Ã£o RÂ² Score')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(results_df['Modelo'], rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE\n",
    "axes[1].bar(x_pos - 0.2, results_df['Train MSE'], 0.4, label='Train', alpha=0.7)\n",
    "axes[1].bar(x_pos + 0.2, results_df['Test MSE'], 0.4, label='Test', alpha=0.7)\n",
    "axes[1].set_xlabel('Modelos')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('ðŸ“Š ComparaÃ§Ã£o MSE')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(results_df['Modelo'], rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_model = regression_results[best_model_name]['model']\n",
    "print(f\"\\nðŸ† Melhor modelo: {best_model_name}\")\n",
    "print(f\"ðŸ“Š Test RÂ²: {results_df.iloc[0]['Test RÂ²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ“ˆ Modelos de ClassificaÃ§Ã£o (DireÃ§Ã£o do PreÃ§o)\n",
    "\n",
    "Vamos treinar modelos para prever a direÃ§Ã£o do movimento dos preÃ§os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos de classificaÃ§Ã£o\n",
    "print(\"ðŸŽ¯ Treinando modelos de classificaÃ§Ã£o...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "classification_results = {}\n",
    "\n",
    "for name, model in classification_models.items():\n",
    "    print(f\"\\nðŸ”§ Treinando {name}...\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model.fit(X_train_scaled, y_dir_train)\n",
    "    \n",
    "    # Fazer prediÃ§Ãµes\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular mÃ©tricas\n",
    "    train_acc = accuracy_score(y_dir_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_dir_test, y_pred_test)\n",
    "    \n",
    "    classification_results[name] = {\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  ðŸ“Š Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Todos os modelos de classificaÃ§Ã£o treinados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar performance dos modelos de classificaÃ§Ã£o\n",
    "print(\"ðŸŽ¯ COMPARAÃ‡ÃƒO DE PERFORMANCE - CLASSIFICAÃ‡ÃƒO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "class_results_df = pd.DataFrame({\n",
    "    'Modelo': list(classification_results.keys()),\n",
    "    'Train Accuracy': [results['train_acc'] for results in classification_results.values()],\n",
    "    'Test Accuracy': [results['test_acc'] for results in classification_results.values()]\n",
    "})\n",
    "\n",
    "# Ordenar por Test Accuracy\n",
    "class_results_df = class_results_df.sort_values('Test Accuracy', ascending=False)\n",
    "display(class_results_df)\n",
    "\n",
    "# Visualizar resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(class_results_df))\n",
    "plt.bar(x_pos - 0.2, class_results_df['Train Accuracy'], 0.4, label='Train', alpha=0.7)\n",
    "plt.bar(x_pos + 0.2, class_results_df['Test Accuracy'], 0.4, label='Test', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('ðŸŽ¯ ComparaÃ§Ã£o Accuracy - ClassificaÃ§Ã£o')\n",
    "plt.xticks(x_pos, class_results_df['Modelo'], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Melhor modelo de classificaÃ§Ã£o\n",
    "best_class_model_name = class_results_df.iloc[0]['Modelo']\n",
    "best_class_model = classification_results[best_class_model_name]['model']\n",
    "print(f\"\\nðŸ† Melhor modelo de classificaÃ§Ã£o: {best_class_model_name}\")\n",
    "print(f\"ðŸŽ¯ Test Accuracy: {class_results_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "\n",
    "# Matriz de confusÃ£o do melhor modelo\n",
    "best_predictions = classification_results[best_class_model_name]['predictions']\n",
    "cm = confusion_matrix(y_dir_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "plt.title(f'ðŸŽ¯ Matriz de ConfusÃ£o - {best_class_model_name}')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('PrediÃ§Ã£o')\n",
    "plt.show()\n",
    "\n",
    "# RelatÃ³rio de classificaÃ§Ã£o\n",
    "print(f\"\\nðŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o - {best_class_model_name}:\")\n",
    "print(classification_report(y_dir_test, best_predictions, target_names=['Down', 'Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ðŸ’­ AnÃ¡lise de Sentimentos\n",
    "\n",
    "Vamos demonstrar anÃ¡lise de sentimentos em textos financeiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar analisador de sentimentos\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "print(\"ðŸ’­ DemonstraÃ§Ã£o de AnÃ¡lise de Sentimentos\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Textos de exemplo relacionados a finanÃ§as\n",
    "sample_texts = [\n",
    "    \"Apple stock surges to new all-time high after strong earnings report\",\n",
    "    \"Market crashes amid fears of recession and inflation concerns\",\n",
    "    \"Tesla announces record deliveries, stock price jumps 15%\",\n",
    "    \"Federal Reserve raises interest rates, markets decline sharply\",\n",
    "    \"Google reports disappointing revenue, shares fall in after-hours trading\",\n",
    "    \"Strong job growth data boosts investor confidence in the economy\",\n",
    "    \"Oil prices plummet due to oversupply concerns and weak demand\",\n",
    "    \"Tech stocks rally on positive AI development news\",\n",
    "    \"Banking sector faces headwinds from regulatory changes\",\n",
    "    \"Cryptocurrency market shows signs of recovery after recent volatility\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“° Analisando sentimentos de notÃ­cias financeiras...\\n\")\n",
    "\n",
    "sentiment_results = []\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    # AnÃ¡lise de sentimento\n",
    "    sentiment_score = sentiment_analyzer.analyze_sentiment(text)\n",
    "    \n",
    "    # Classificar sentimento\n",
    "    if sentiment_score > 0.1:\n",
    "        sentiment_label = \"ðŸ“ˆ POSITIVO\"\n",
    "        color = \"green\"\n",
    "    elif sentiment_score < -0.1:\n",
    "        sentiment_label = \"ðŸ“‰ NEGATIVO\"\n",
    "        color = \"red\"\n",
    "    else:\n",
    "        sentiment_label = \"âž– NEUTRO\"\n",
    "        color = \"gray\"\n",
    "    \n",
    "    sentiment_results.append({\n",
    "        'text': text,\n",
    "        'score': sentiment_score,\n",
    "        'label': sentiment_label\n",
    "    })\n",
    "    \n",
    "    print(f\"{i:2d}. {sentiment_label} ({sentiment_score:+.3f})\")\n",
    "    print(f\"    '{text}'\\n\")\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "sentiment_df = pd.DataFrame(sentiment_results)\n",
    "\n",
    "print(\"ðŸ“Š DistribuiÃ§Ã£o de Sentimentos:\")\n",
    "sentiment_counts = sentiment_df['label'].value_counts()\n",
    "for label, count in sentiment_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(sentiment_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuiÃ§Ã£o de sentimentos\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histograma de scores\n",
    "axes[0].hist(sentiment_df['score'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Neutro')\n",
    "axes[0].set_xlabel('Score de Sentimento')\n",
    "axes[0].set_ylabel('FrequÃªncia')\n",
    "axes[0].set_title('ðŸ“Š DistribuiÃ§Ã£o dos Scores de Sentimento')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico de barras por categoria\n",
    "sentiment_counts.plot(kind='bar', ax=axes[1], color=['red', 'gray', 'green'], alpha=0.7)\n",
    "axes[1].set_xlabel('Categoria de Sentimento')\n",
    "axes[1].set_ylabel('Quantidade')\n",
    "axes[1].set_title('ðŸ“Š Contagem por Categoria')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Score mÃ©dio: {sentiment_df['score'].mean():.3f}\")\n",
    "print(f\"ðŸ“Š Desvio padrÃ£o: {sentiment_df['score'].std():.3f}\")\n",
    "print(f\"ðŸ“‰ Score mÃ­nimo: {sentiment_df['score'].min():.3f}\")\n",
    "print(f\"ðŸ“ˆ Score mÃ¡ximo: {sentiment_df['score'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ”— CorrelaÃ§Ã£o Sentimento vs PreÃ§os\n",
    "\n",
    "Vamos simular a correlaÃ§Ã£o entre sentimentos e movimentos de preÃ§os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular sÃ©rie temporal de sentimentos\n",
    "print(\"ðŸ”— Simulando correlaÃ§Ã£o Sentimento vs PreÃ§os\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Pegar dados de um sÃ­mbolo especÃ­fico\n",
    "symbol = 'AAPL'\n",
    "symbol_data = df_enhanced[df_enhanced['symbol'] == symbol].copy().sort_values('date')\n",
    "\n",
    "# Simular sentimentos diÃ¡rios (correlacionados com retornos)\n",
    "np.random.seed(42)\n",
    "returns = symbol_data['close'].pct_change().fillna(0)\n",
    "\n",
    "# Criar sentimentos com alguma correlaÃ§Ã£o com retornos + ruÃ­do\n",
    "sentiment_scores = []\n",
    "for i, ret in enumerate(returns):\n",
    "    # Sentimento baseado no retorno + ruÃ­do\n",
    "    base_sentiment = ret * 2  # Amplificar correlaÃ§Ã£o\n",
    "    noise = np.random.normal(0, 0.1)  # Adicionar ruÃ­do\n",
    "    sentiment = base_sentiment + noise\n",
    "    \n",
    "    # Limitar entre -1 e 1\n",
    "    sentiment = np.clip(sentiment, -1, 1)\n",
    "    sentiment_scores.append(sentiment)\n",
    "\n",
    "symbol_data['sentiment'] = sentiment_scores\n",
    "symbol_data['returns'] = returns\n",
    "\n",
    "# Calcular correlaÃ§Ã£o\n",
    "correlation = symbol_data['sentiment'].corr(symbol_data['returns'])\n",
    "print(f\"ðŸ“Š CorrelaÃ§Ã£o Sentimento vs Retornos: {correlation:.3f}\")\n",
    "\n",
    "# AnÃ¡lise de lag (sentimento pode preceder movimento de preÃ§os)\n",
    "lag_correlations = []\n",
    "for lag in range(0, 6):\n",
    "    if lag == 0:\n",
    "        corr = symbol_data['sentiment'].corr(symbol_data['returns'])\n",
    "    else:\n",
    "        corr = symbol_data['sentiment'].corr(symbol_data['returns'].shift(-lag))\n",
    "    lag_correlations.append(corr)\n",
    "    print(f\"ðŸ“ˆ CorrelaÃ§Ã£o com lag {lag}: {corr:.3f}\")\n",
    "\n",
    "# Visualizar correlaÃ§Ãµes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# SÃ©rie temporal de sentimentos e preÃ§os\n",
    "ax1 = axes[0, 0]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "dates = symbol_data['date'].iloc[-100:]  # Ãšltimos 100 dias\n",
    "sentiment_recent = symbol_data['sentiment'].iloc[-100:]\n",
    "price_recent = symbol_data['close'].iloc[-100:]\n",
    "\n",
    "line1 = ax1.plot(dates, sentiment_recent, 'b-', alpha=0.7, label='Sentimento')\n",
    "line2 = ax2.plot(dates, price_recent, 'r-', alpha=0.7, label='PreÃ§o')\n",
    "\n",
    "ax1.set_xlabel('Data')\n",
    "ax1.set_ylabel('Score de Sentimento', color='b')\n",
    "ax2.set_ylabel('PreÃ§o ($)', color='r')\n",
    "ax1.set_title(f'ðŸ“Š Sentimento vs PreÃ§o - {symbol}')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Scatter plot\n",
    "axes[0, 1].scatter(symbol_data['sentiment'], symbol_data['returns'], alpha=0.6)\n",
    "axes[0, 1].set_xlabel('Score de Sentimento')\n",
    "axes[0, 1].set_ylabel('Retorno DiÃ¡rio')\n",
    "axes[0, 1].set_title(f'ðŸ”— CorrelaÃ§Ã£o: {correlation:.3f}')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CorrelaÃ§Ãµes com lag\n",
    "axes[1, 0].bar(range(len(lag_correlations)), lag_correlations, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Lag (dias)')\n",
    "axes[1, 0].set_ylabel('CorrelaÃ§Ã£o')\n",
    "axes[1, 0].set_title('ðŸ“ˆ CorrelaÃ§Ã£o por Lag')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# DistribuiÃ§Ã£o de sentimentos\n",
    "axes[1, 1].hist(symbol_data['sentiment'], bins=20, alpha=0.7, color='green')\n",
    "axes[1, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Score de Sentimento')\n",
    "axes[1, 1].set_ylabel('FrequÃªncia')\n",
    "axes[1, 1].set_title('ðŸ“Š DistribuiÃ§Ã£o de Sentimentos')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸ” Interpretabilidade dos Modelos\n",
    "\n",
    "Vamos analisar quais features sÃ£o mais importantes nos nossos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lise de importÃ¢ncia das features\n",
    "print(\"ðŸ” AnÃ¡lise de ImportÃ¢ncia das Features\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Usar Random Forest para anÃ¡lise de importÃ¢ncia\n",
    "rf_reg = regression_results['Random Forest']['model']\n",
    "rf_class = classification_results['Random Forest']['model']\n",
    "\n",
    "# ImportÃ¢ncia para regressÃ£o\n",
    "feature_importance_reg = rf_reg.feature_importances_\n",
    "feature_importance_class = rf_class.feature_importances_\n",
    "\n",
    "# Criar DataFrame com importÃ¢ncias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names[:len(feature_importance_reg)],\n",
    "    'Importance_Regression': feature_importance_reg,\n",
    "    'Importance_Classification': feature_importance_class\n",
    "})\n",
    "\n",
    "# Ordenar por importÃ¢ncia mÃ©dia\n",
    "importance_df['Importance_Mean'] = (importance_df['Importance_Regression'] + \n",
    "                                  importance_df['Importance_Classification']) / 2\n",
    "importance_df = importance_df.sort_values('Importance_Mean', ascending=False)\n",
    "\n",
    "# Top 15 features mais importantes\n",
    "top_features = importance_df.head(15)\n",
    "\n",
    "print(\"ðŸ† Top 15 Features Mais Importantes:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['Feature']:<25} | Reg: {row['Importance_Regression']:.4f} | Class: {row['Importance_Classification']:.4f}\")\n",
    "\n",
    "# Visualizar importÃ¢ncias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# ImportÃ¢ncia para regressÃ£o\n",
    "top_features.plot(x='Feature', y='Importance_Regression', kind='barh', \n",
    "                 ax=axes[0], color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('ðŸ“Š ImportÃ¢ncia das Features - RegressÃ£o')\n",
    "axes[0].set_xlabel('ImportÃ¢ncia')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ImportÃ¢ncia para classificaÃ§Ã£o\n",
    "top_features.plot(x='Feature', y='Importance_Classification', kind='barh', \n",
    "                 ax=axes[1], color='coral', alpha=0.7)\n",
    "axes[1].set_title('ðŸŽ¯ ImportÃ¢ncia das Features - ClassificaÃ§Ã£o')\n",
    "axes[1].set_xlabel('ImportÃ¢ncia')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. âš¡ ValidaÃ§Ã£o Cruzada Temporal\n",
    "\n",
    "Vamos fazer uma validaÃ§Ã£o mais robusta usando Time Series Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValidaÃ§Ã£o cruzada temporal\n",
    "print(\"âš¡ ValidaÃ§Ã£o Cruzada Temporal\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Configurar Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Testar melhor modelo de cada tipo\n",
    "models_to_validate = {\n",
    "    'Best Regression': regression_results[best_model_name]['model'],\n",
    "    'Best Classification': classification_results[best_class_model_name]['model']\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for model_name, model in models_to_validate.items():\n",
    "    print(f\"\\nðŸ”§ Validando {model_name}...\")\n",
    "    \n",
    "    if 'Regression' in model_name:\n",
    "        # ValidaÃ§Ã£o para regressÃ£o\n",
    "        scores = cross_val_score(model, X_train_scaled, y_price_train, \n",
    "                               cv=tscv, scoring='r2')\n",
    "        metric_name = 'RÂ²'\n",
    "    else:\n",
    "        # ValidaÃ§Ã£o para classificaÃ§Ã£o\n",
    "        scores = cross_val_score(model, X_train_scaled, y_dir_train, \n",
    "                               cv=tscv, scoring='accuracy')\n",
    "        metric_name = 'Accuracy'\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'scores': scores,\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'metric': metric_name\n",
    "    }\n",
    "    \n",
    "    print(f\"  ðŸ“Š {metric_name} mÃ©dio: {scores.mean():.4f} (Â±{scores.std():.4f})\")\n",
    "    print(f\"  ðŸ“ˆ Scores por fold: {[f'{s:.4f}' for s in scores]}\")\n",
    "\n",
    "# Visualizar resultados da validaÃ§Ã£o cruzada\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, (model_name, results) in enumerate(cv_results.items()):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    \n",
    "    scores = results['scores']\n",
    "    folds = range(1, len(scores) + 1)\n",
    "    \n",
    "    plt.plot(folds, scores, 'o-', linewidth=2, markersize=8, alpha=0.7)\n",
    "    plt.axhline(y=results['mean'], color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'MÃ©dia: {results[\"mean\"]:.4f}')\n",
    "    \n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel(results['metric'])\n",
    "    plt.title(f'ðŸ“Š ValidaÃ§Ã£o Cruzada - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(folds)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… ValidaÃ§Ã£o cruzada concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ðŸ“Š RelatÃ³rio Final e Insights\n",
    "\n",
    "Vamos gerar um relatÃ³rio final com todos os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relatÃ³rio final\n",
    "print(\"ðŸ“Š RELATÃ“RIO FINAL - MACHINE LEARNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DADOS PROCESSADOS:\")\n",
    "print(f\"  â€¢ Total de amostras: {X_combined.shape[0]:,}\")\n",
    "print(f\"  â€¢ Features criadas: {X_combined.shape[1]}\")\n",
    "print(f\"  â€¢ SÃ­mbolos analisados: {len(df_enhanced['symbol'].unique())}\")\n",
    "print(f\"  â€¢ PerÃ­odo: {df_enhanced['date'].min().strftime('%Y-%m-%d')} atÃ© {df_enhanced['date'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nðŸ¤– MODELOS DE REGRESSÃƒO:\")\n",
    "print(f\"  ðŸ† Melhor modelo: {best_model_name}\")\n",
    "print(f\"  ðŸ“Š Test RÂ²: {results_df.iloc[0]['Test RÂ²']:.4f}\")\n",
    "print(f\"  ðŸ“Š Test MSE: {results_df.iloc[0]['Test MSE']:.4f}\")\n",
    "print(f\"  âš¡ CV RÂ² mÃ©dio: {cv_results['Best Regression']['mean']:.4f} (Â±{cv_results['Best Regression']['std']:.4f})\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MODELOS DE CLASSIFICAÃ‡ÃƒO:\")\n",
    "print(f\"  ðŸ† Melhor modelo: {best_class_model_name}\")\n",
    "print(f\"  ðŸ“Š Test Accuracy: {class_results_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "print(f\"  âš¡ CV Accuracy mÃ©dio: {cv_results['Best Classification']['mean']:.4f} (Â±{cv_results['Best Classification']['std']:.4f})\")\n",
    "\n",
    "print(f\"\\nðŸ’­ ANÃLISE DE SENTIMENTOS:\")\n",
    "print(f\"  ðŸ“Š Textos analisados: {len(sentiment_df)}\")\n",
    "print(f\"  ðŸ“ˆ Score mÃ©dio: {sentiment_df['score'].mean():.3f}\")\n",
    "print(f\"  ðŸ”— CorrelaÃ§Ã£o simulada: {correlation:.3f}\")\n",
    "print(f\"  ðŸ“Š DistribuiÃ§Ã£o: {dict(sentiment_counts)}\")\n",
    "\n",
    "print(f\"\\nðŸ” FEATURES MAIS IMPORTANTES:\")\n",
    "for i, (_, row) in enumerate(top_features.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Feature']} (ImportÃ¢ncia: {row['Importance_Mean']:.4f})\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ INSIGHTS E RECOMENDAÃ‡Ã•ES:\")\n",
    "print(f\"  â€¢ Os modelos ensemble (Random Forest, Gradient Boosting) geralmente performam melhor\")\n",
    "print(f\"  â€¢ Features tÃ©cnicas (mÃ©dias mÃ³veis, volatilidade) sÃ£o muito importantes\")\n",
    "print(f\"  â€¢ A validaÃ§Ã£o cruzada temporal Ã© essencial para dados financeiros\")\n",
    "print(f\"  â€¢ Sentimentos podem ser um indicador Ãºtil quando combinados com dados tÃ©cnicos\")\n",
    "print(f\"  â€¢ Ã‰ importante considerar o lag entre sentimentos e movimentos de preÃ§os\")\n",
    "\n",
    "print(f\"\\nðŸ”„ PRÃ“XIMOS PASSOS:\")\n",
    "print(f\"  1. ðŸ“Š Implementar mais features de sentimento em tempo real\")\n",
    "print(f\"  2. ðŸ”§ Otimizar hiperparÃ¢metros dos modelos\")\n",
    "print(f\"  3. ðŸ“ˆ Criar ensemble de modelos\")\n",
    "print(f\"  4. ðŸŽ¯ Implementar estratÃ©gias de trading baseadas nas prediÃ§Ãµes\")\n",
    "print(f\"  5. ðŸ“Š Monitorar performance em produÃ§Ã£o\")\n",
    "\n",
    "print(f\"\\nâœ¨ AnÃ¡lise de Machine Learning concluÃ­da com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ðŸ’¾ Salvamento dos Modelos\n",
    "\n",
    "Vamos salvar os melhores modelos para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos e resultados\n",
    "import joblib\n",
    "\n",
    "print(\"ðŸ’¾ Salvando modelos e resultados...\")\n",
    "\n",
    "# Criar diretÃ³rio de modelos\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Salvar melhores modelos\n",
    "joblib.dump(best_model, f'{models_dir}/best_regression_model.pkl')\n",
    "joblib.dump(best_class_model, f'{models_dir}/best_classification_model.pkl')\n",
    "joblib.dump(scaler, f'{models_dir}/feature_scaler.pkl')\n",
    "\n",
    "print(f\"âœ… Modelos salvos em: {models_dir}\")\n",
    "\n",
    "# Salvar resultados\n",
    "results_dir = '../data/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Salvar DataFrames de resultados\n",
    "results_df.to_csv(f'{results_dir}/regression_results.csv', index=False)\n",
    "class_results_df.to_csv(f'{results_dir}/classification_results.csv', index=False)\n",
    "importance_df.to_csv(f'{results_dir}/feature_importance.csv', index=False)\n",
    "sentiment_df.to_csv(f'{results_dir}/sentiment_analysis.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Resultados salvos em: {results_dir}\")\n",
    "\n",
    "# Salvar configuraÃ§Ãµes\n",
    "config = {\n",
    "    'best_regression_model': best_model_name,\n",
    "    'best_classification_model': best_class_model_name,\n",
    "    'feature_names': feature_names,\n",
    "    'symbols_analyzed': list(df_enhanced['symbol'].unique()),\n",
    "    'date_range': {\n",
    "        'start': df_enhanced['date'].min().strftime('%Y-%m-%d'),\n",
    "        'end': df_enhanced['date'].max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'performance': {\n",
    "        'regression_r2': float(results_df.iloc[0]['Test RÂ²']),\n",
    "        'classification_accuracy': float(class_results_df.iloc[0]['Test Accuracy']),\n",
    "        'sentiment_correlation': float(correlation)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{results_dir}/ml_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"âœ… ConfiguraÃ§Ãµes salvas em: {results_dir}/ml_config.json\")\n",
    "print(f\"\\nðŸ“ Arquivos criados:\")\n",
    "print(f\"  â€¢ {models_dir}/best_regression_model.pkl\")\n",
    "print(f\"  â€¢ {models_dir}/best_classification_model.pkl\")\n",
    "print(f\"  â€¢ {models_dir}/feature_scaler.pkl\")\n",
    "print(f\"  â€¢ {results_dir}/regression_results.csv\")\n",
    "print(f\"  â€¢ {results_dir}/classification_results.csv\")\n",
    "print(f\"  â€¢ {results_dir}/feature_importance.csv\")\n",
    "print(f\"  â€¢ {results_dir}/sentiment_analysis.csv\")\n",
    "print(f\"  â€¢ {results_dir}/ml_config.json\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Notebook de Machine Learning concluÃ­do com sucesso!\")\n",
    "print(f\"ðŸ“Š Execute o prÃ³ximo notebook para visualizaÃ§Ãµes interativas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Resumo do Notebook\n",
    "\n",
    "Este notebook demonstrou:\n",
    "\n",
    "### âœ… **Funcionalidades Implementadas:**\n",
    "- ðŸ”§ **Engenharia de Features**: Indicadores tÃ©cnicos, lags, rolling features\n",
    "- ðŸ¤– **Modelos de RegressÃ£o**: PrediÃ§Ã£o de preÃ§os futuros\n",
    "- ðŸŽ¯ **Modelos de ClassificaÃ§Ã£o**: PrediÃ§Ã£o da direÃ§Ã£o dos preÃ§os\n",
    "- ðŸ’­ **AnÃ¡lise de Sentimentos**: Processamento de textos financeiros\n",
    "- ðŸ”— **CorrelaÃ§Ã£o**: Sentimentos vs movimentos de preÃ§os\n",
    "- ðŸ” **Interpretabilidade**: AnÃ¡lise de importÃ¢ncia das features\n",
    "- âš¡ **ValidaÃ§Ã£o Robusta**: Time Series Cross-Validation\n",
    "- ðŸ’¾ **PersistÃªncia**: Salvamento de modelos e resultados\n",
    "\n",
    "### ðŸ“Š **Modelos Testados:**\n",
    "- **RegressÃ£o**: Linear, Ridge, Lasso, Random Forest, Gradient Boosting, SVR\n",
    "- **ClassificaÃ§Ã£o**: Logistic Regression, Random Forest, Gradient Boosting, SVM, Naive Bayes\n",
    "- **Sentimentos**: TextBlob, VADER, anÃ¡lise financeira customizada\n",
    "\n",
    "### ðŸŽ¯ **PrÃ³ximos Passos:**\n",
    "1. Execute o notebook `04_interactive_dashboard.ipynb` para visualizaÃ§Ãµes\n",
    "2. Implemente estratÃ©gias de trading baseadas nas prediÃ§Ãµes\n",
    "3. Configure monitoramento em tempo real\n",
    "4. Otimize hiperparÃ¢metros dos modelos\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido por:** Anny Caroline Sousa & Equipe Big Data Finance  \n",
    "**Projeto:** Sistema de AnÃ¡lise Financeira com Big Data  \n",
    "**VersÃ£o:** 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos e resultados\n",
    "import joblib\n",
    "\n",
    "print(\"ðŸ’¾ Salvando modelos e resultados...\")\n",
    "\n",
    "# Criar diretÃ³rio de modelos\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Salvar melhores modelos\n",
    "joblib.dump(best_model, f'{models_dir}/best_regression_model.pkl')\n",
    "joblib.dump(best_class_model, f'{models_dir}/best_classification_model.pkl')\n",
    "joblib.dump(scaler, f'{models_dir}/feature_scaler.pkl')\n",
    "\n",
    "print(f\"âœ… Modelos salvos em: {models_dir}\")\n",
    "\n",
    "# Salvar resultados\n",
    "results_dir = '../data/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Salvar DataFrames de resultados\n",
    "results_df.to_csv(f'{results_dir}/regression_results.csv', index=False)\n",
    "class_results_df.to_csv(f'{results_dir}/classification_results.csv', index=False)\n",
    "importance_df.to_csv(f'{results_dir}/feature_importance.csv', index=False)\n",
    "sentiment_df.to_csv(f'{results_dir}/sentiment_analysis.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Resultados salvos em: {results_dir}\")\n",
    "\n",
    "# Salvar configuraÃ§Ãµes\n",
    "config = {\n",
    "    'best_regression_model': best_model_name,\n",
    "    'best_classification_model': best_class_model_name,\n",
    "    'feature_names': feature_names,\n",
    "    'symbols_analyzed': list(df_enhanced['symbol'].unique()),\n",
    "    'date_range': {\n",
    "        'start': df_enhanced['date'].min().strftime('%Y-%m-%d'),\n",
    "        'end': df_enhanced['date'].max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'performance': {\n",
    "        'regression_r2': float(results_df.iloc[0]['Test RÂ²']),\n",
    "        'classification_accuracy': float(class_results_df.iloc[0]['Test Accuracy']),\n",
    "        'sentiment_correlation': float(correlation)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{results_dir}/ml_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"âœ… ConfiguraÃ§Ãµes salvas em: {results_dir}/ml_config.json\")\n",
    "print(f\"\\nðŸ“ Arquivos criados:\")\n",
    "print(f\"  â€¢ {models_dir}/best_regression_model.pkl\")\n",
    "print(f\"  â€¢ {models_dir}/best_classification_model.pkl\")\n",
    "print(f\"  â€¢ {models_dir}/feature_scaler.pkl\")\n",
    "print(f\"  â€¢ {results_dir}/regression_results.csv\")\n",
    "print(f\"  â€¢ {results_dir}/classification_results.csv\")\n",
    "print(f\"  â€¢ {results_dir}/feature_importance.csv\")\n",
    "print(f\"  â€¢ {results_dir}/sentiment_analysis.csv\")\n",
    "print(f\"  â€¢ {results_dir}/ml_config.json\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Notebook de Machine Learning concluÃ­do com sucesso!\")\n",
    "print(f\"ðŸ“Š Execute o prÃ³ximo notebook para visualizaÃ§Ãµes interativas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source