{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Machine Learning e Análise de Sentimentos - Big Data Finance\n",
    "\n",
    "Este notebook demonstra como aplicar técnicas de Machine Learning e análise de sentimentos aos dados financeiros.\n",
    "\n",
    "## Objetivos\n",
    "- Engenharia de features para dados financeiros\n",
    "- Modelos preditivos para preços e direção\n",
    "- Análise de sentimentos de notícias\n",
    "- Correlação entre sentimentos e preços\n",
    "- Avaliação e otimização de modelos\n",
    "- Interpretabilidade dos resultados\n",
    "\n",
    "**Autor:** Anny Caroline Sousa (Machine Learning) & Equipe Big Data Finance  \n",
    "**Gestão:** Fabio  \n",
    "**Infraestrutura:** Ana Luiza Pazze  \n",
    "**Data:** 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Imports de ML\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Imports dos módulos do projeto\n",
    "from machine_learning.sentiment_analyzer import SentimentAnalyzer\n",
    "from machine_learning.predictive_models import PredictiveModels, FeatureEngineer\n",
    "from data_analysis.statistical_analyzer import StatisticalAnalyzer\n",
    "\n",
    "print(\"✅ Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📊 Carregamento e Preparação dos Dados\n",
    "\n",
    "Vamos carregar os dados processados pelo Spark e preparar para ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar dados processados disponíveis\n",
    "processed_dir = '../data/processed'\n",
    "raw_dir = '../data/raw'\n",
    "\n",
    "# Tentar carregar dados do Spark primeiro\n",
    "if os.path.exists(f\"{processed_dir}/stock_indicators_csv\"):\n",
    "    print(\"📊 Carregando dados processados pelo Spark...\")\n",
    "    # Encontrar arquivo CSV dentro do diretório\n",
    "    csv_files = [f for f in os.listdir(f\"{processed_dir}/stock_indicators_csv\") if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        df = pd.read_csv(f\"{processed_dir}/stock_indicators_csv/{csv_files[0]}\")\n",
    "        print(f\"✅ Dados carregados: {len(df)} registros\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Arquivo CSV não encontrado no diretório processado\")\n",
    "else:\n",
    "    print(\"⚠️ Dados processados não encontrados. Carregando dados brutos...\")\n",
    "    # Carregar dados brutos\n",
    "    stock_files = [f for f in os.listdir(raw_dir) if f.startswith('stock_data_') and f.endswith('.csv')]\n",
    "    if stock_files:\n",
    "        latest_file = sorted(stock_files)[-1]\n",
    "        df = pd.read_csv(f\"{raw_dir}/{latest_file}\")\n",
    "        print(f\"📁 Carregando dados brutos: {latest_file}\")\n",
    "        print(f\"✅ Dados carregados: {len(df)} registros\")\n",
    "    else:\n",
    "        print(\"🔧 Criando dados sintéticos para demonstração...\")\n",
    "        # Criar dados sintéticos\n",
    "        dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='D')\n",
    "        symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "        \n",
    "        data = []\n",
    "        for symbol in symbols:\n",
    "            base_price = np.random.uniform(100, 300)\n",
    "            for i, date in enumerate(dates):\n",
    "                price_change = np.random.normal(0, 0.02)\n",
    "                base_price *= (1 + price_change)\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date.strftime('%Y-%m-%d'),\n",
    "                    'symbol': symbol,\n",
    "                    'open': base_price * np.random.uniform(0.98, 1.02),\n",
    "                    'high': base_price * np.random.uniform(1.00, 1.05),\n",
    "                    'low': base_price * np.random.uniform(0.95, 1.00),\n",
    "                    'close': base_price,\n",
    "                    'volume': np.random.randint(1000000, 10000000)\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"✅ Dados sintéticos criados: {len(df)} registros\")\n",
    "\n",
    "# Converter data\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"\\n📋 Colunas disponíveis: {list(df.columns)}\")\n",
    "print(f\"📅 Período: {df['date'].min()} até {df['date'].max()}\")\n",
    "print(f\"📈 Símbolos: {df['symbol'].unique()}\")\n",
    "\n",
    "# Visualizar primeiros registros\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🔧 Engenharia de Features\n",
    "\n",
    "Vamos criar features técnicas para os modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Feature Engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "print(\"🔧 Criando features técnicas...\")\n",
    "\n",
    "# Processar cada símbolo separadamente\n",
    "enhanced_data = []\n",
    "\n",
    "for symbol in df['symbol'].unique():\n",
    "    print(f\"📊 Processando {symbol}...\")\n",
    "    \n",
    "    # Filtrar dados do símbolo\n",
    "    symbol_data = df[df['symbol'] == symbol].copy().sort_values('date')\n",
    "    \n",
    "    # Criar indicadores técnicos\n",
    "    symbol_data = feature_engineer.create_technical_indicators(symbol_data)\n",
    "    \n",
    "    # Criar features de lag\n",
    "    symbol_data = feature_engineer.create_lag_features(symbol_data, ['close', 'volume'], lags=[1, 2, 3, 5])\n",
    "    \n",
    "    # Criar features de rolling\n",
    "    symbol_data = feature_engineer.create_rolling_features(symbol_data, ['close', 'volume'], windows=[5, 10, 20])\n",
    "    \n",
    "    enhanced_data.append(symbol_data)\n",
    "\n",
    "# Combinar todos os dados\n",
    "df_enhanced = pd.concat(enhanced_data, ignore_index=True)\n",
    "\n",
    "# Remover NaN (devido aos lags e rolling)\n",
    "df_enhanced = df_enhanced.dropna()\n",
    "\n",
    "print(f\"✅ Features criadas: {len(df_enhanced.columns)} colunas\")\n",
    "print(f\"📊 Registros após limpeza: {len(df_enhanced)}\")\n",
    "\n",
    "# Mostrar novas features\n",
    "new_features = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "print(f\"\\n🆕 Novas features ({len(new_features)}):\")\n",
    "for i, feature in enumerate(new_features[:15]):  # Mostrar apenas as primeiras 15\n",
    "    print(f\"  {i+1:2d}. {feature}\")\n",
    "if len(new_features) > 15:\n",
    "    print(f\"  ... e mais {len(new_features) - 15} features\")\n",
    "\n",
    "display(df_enhanced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🎯 Preparação dos Targets\n",
    "\n",
    "Vamos criar diferentes targets para nossos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Predictive Models\n",
    "predictive_models = PredictiveModels()\n",
    "\n",
    "print(\"🎯 Criando targets para predição...\")\n",
    "\n",
    "# Preparar dados para cada símbolo\n",
    "model_data = {}\n",
    "\n",
    "for symbol in df_enhanced['symbol'].unique():\n",
    "    print(f\"\\n📊 Preparando dados para {symbol}...\")\n",
    "    \n",
    "    # Filtrar dados do símbolo\n",
    "    symbol_data = df_enhanced[df_enhanced['symbol'] == symbol].copy().sort_values('date')\n",
    "    \n",
    "    # Preparar dados\n",
    "    X, y_price, y_direction, y_volatility = predictive_models.prepare_data(\n",
    "        symbol_data, \n",
    "        target_days=1\n",
    "    )\n",
    "    \n",
    "    model_data[symbol] = {\n",
    "        'X': X,\n",
    "        'y_price': y_price,\n",
    "        'y_direction': y_direction,\n",
    "        'y_volatility': y_volatility,\n",
    "        'dates': symbol_data['date'].iloc[:-1].values  # Excluir última data (sem target)\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✅ Features: {X.shape[1]} | Amostras: {X.shape[0]}\")\n",
    "    print(f\"  📈 Distribuição direção: {np.bincount(y_direction)}\")\n",
    "\n",
    "# Combinar dados de todos os símbolos\n",
    "X_combined = np.vstack([data['X'] for data in model_data.values()])\n",
    "y_price_combined = np.hstack([data['y_price'] for data in model_data.values()])\n",
    "y_direction_combined = np.hstack([data['y_direction'] for data in model_data.values()])\n",
    "y_volatility_combined = np.hstack([data['y_volatility'] for data in model_data.values()])\n",
    "\n",
    "print(f\"\\n📊 DADOS COMBINADOS:\")\n",
    "print(f\"  📈 Total de amostras: {X_combined.shape[0]:,}\")\n",
    "print(f\"  🔧 Total de features: {X_combined.shape[1]}\")\n",
    "print(f\"  📊 Distribuição direção geral: {np.bincount(y_direction_combined)}\")\n",
    "\n",
    "# Nomes das features\n",
    "feature_names = [col for col in df_enhanced.columns if col not in ['date', 'symbol']]\n",
    "print(f\"\\n📋 Features principais:\")\n",
    "for i, name in enumerate(feature_names[:10]):\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "if len(feature_names) > 10:\n",
    "    print(f\"  ... e mais {len(feature_names) - 10} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 🤖 Modelos de Regressão (Predição de Preços)\n",
    "\n",
    "Vamos treinar modelos para prever preços futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dados para treino e teste\n",
    "print(\"🔄 Dividindo dados para treino e teste...\")\n",
    "\n",
    "# Usar 80% para treino, 20% para teste (mantendo ordem temporal)\n",
    "split_idx = int(0.8 * len(X_combined))\n",
    "\n",
    "X_train, X_test = X_combined[:split_idx], X_combined[split_idx:]\n",
    "y_price_train, y_price_test = y_price_combined[:split_idx], y_price_combined[split_idx:]\n",
    "y_dir_train, y_dir_test = y_direction_combined[:split_idx], y_direction_combined[split_idx:]\n",
    "\n",
    "print(f\"📊 Treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"📊 Teste: {X_test.shape[0]:,} amostras\")\n",
    "\n",
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✅ Dados normalizados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos de regressão\n",
    "print(\"🤖 Treinando modelos de regressão...\")\n",
    "\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n🔧 Treinando {name}...\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model.fit(X_train_scaled, y_price_train)\n",
    "    \n",
    "    # Fazer predições\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    train_mse = mean_squared_error(y_price_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_price_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_price_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_price_test, y_pred_test)\n",
    "    \n",
    "    regression_results[name] = {\n",
    "        'model': model,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  📊 Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  📊 Train MSE: {train_mse:.4f} | Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Todos os modelos de regressão treinados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar performance dos modelos\n",
    "print(\"📊 COMPARAÇÃO DE PERFORMANCE - REGRESSÃO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': list(regression_results.keys()),\n",
    "    'Train R²': [results['train_r2'] for results in regression_results.values()],\n",
    "    'Test R²': [results['test_r2'] for results in regression_results.values()],\n",
    "    'Train MSE': [results['train_mse'] for results in regression_results.values()],\n",
    "    'Test MSE': [results['test_mse'] for results in regression_results.values()]\n",
    "})\n",
    "\n",
    "# Ordenar por Test R²\n",
    "results_df = results_df.sort_values('Test R²', ascending=False)\n",
    "display(results_df)\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# R² Score\n",
    "x_pos = np.arange(len(results_df))\n",
    "axes[0].bar(x_pos - 0.2, results_df['Train R²'], 0.4, label='Train', alpha=0.7)\n",
    "axes[0].bar(x_pos + 0.2, results_df['Test R²'], 0.4, label='Test', alpha=0.7)\n",
    "axes[0].set_xlabel('Modelos')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('📊 Comparação R² Score')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(results_df['Modelo'], rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE\n",
    "axes[1].bar(x_pos - 0.2, results_df['Train MSE'], 0.4, label='Train', alpha=0.7)\n",
    "axes[1].bar(x_pos + 0.2, results_df['Test MSE'], 0.4, label='Test', alpha=0.7)\n",
    "axes[1].set_xlabel('Modelos')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('📊 Comparação MSE')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(results_df['Modelo'], rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_model = regression_results[best_model_name]['model']\n",
    "print(f\"\\n🏆 Melhor modelo: {best_model_name}\")\n",
    "print(f\"📊 Test R²: {results_df.iloc[0]['Test R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 📈 Modelos de Classificação (Direção do Preço)\n",
    "\n",
    "Vamos treinar modelos para prever a direção do movimento dos preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos de classificação\n",
    "print(\"🎯 Treinando modelos de classificação...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "classification_results = {}\n",
    "\n",
    "for name, model in classification_models.items():\n",
    "    print(f\"\\n🔧 Treinando {name}...\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model.fit(X_train_scaled, y_dir_train)\n",
    "    \n",
    "    # Fazer predições\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    train_acc = accuracy_score(y_dir_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_dir_test, y_pred_test)\n",
    "    \n",
    "    classification_results[name] = {\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  📊 Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Todos os modelos de classificação treinados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar performance dos modelos de classificação\n",
    "print(\"🎯 COMPARAÇÃO DE PERFORMANCE - CLASSIFICAÇÃO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "class_results_df = pd.DataFrame({\n",
    "    'Modelo': list(classification_results.keys()),\n",
    "    'Train Accuracy': [results['train_acc'] for results in classification_results.values()],\n",
    "    'Test Accuracy': [results['test_acc'] for results in classification_results.values()]\n",
    "})\n",
    "\n",
    "# Ordenar por Test Accuracy\n",
    "class_results_df = class_results_df.sort_values('Test Accuracy', ascending=False)\n",
    "display(class_results_df)\n",
    "\n",
    "# Visualizar resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(class_results_df))\n",
    "plt.bar(x_pos - 0.2, class_results_df['Train Accuracy'], 0.4, label='Train', alpha=0.7)\n",
    "plt.bar(x_pos + 0.2, class_results_df['Test Accuracy'], 0.4, label='Test', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('🎯 Comparação Accuracy - Classificação')\n",
    "plt.xticks(x_pos, class_results_df['Modelo'], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Melhor modelo de classificação\n",
    "best_class_model_name = class_results_df.iloc[0]['Modelo']\n",
    "best_class_model = classification_results[best_class_model_name]['model']\n",
    "print(f\"\\n🏆 Melhor modelo de classificação: {best_class_model_name}\")\n",
    "print(f\"🎯 Test Accuracy: {class_results_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "\n",
    "# Matriz de confusão do melhor modelo\n",
    "best_predictions = classification_results[best_class_model_name]['predictions']\n",
    "cm = confusion_matrix(y_dir_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "plt.title(f'🎯 Matriz de Confusão - {best_class_model_name}')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Predição')\n",
    "plt.show()\n",
    "\n",
    "# Relatório de classificação\n",
    "print(f\"\\n📊 Relatório de Classificação - {best_class_model_name}:\")\n",
    "print(classification_report(y_dir_test, best_predictions, target_names=['Down', 'Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 💭 Análise de Sentimentos\n",
    "\n",
    "Vamos demonstrar análise de sentimentos em textos financeiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar analisador de sentimentos\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "print(\"💭 Demonstração de Análise de Sentimentos\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Textos de exemplo relacionados a finanças\n",
    "sample_texts = [\n",
    "    \"Apple stock surges to new all-time high after strong earnings report\",\n",
    "    \"Market crashes amid fears of recession and inflation concerns\",\n",
    "    \"Tesla announces record deliveries, stock price jumps 15%\",\n",
    "    \"Federal Reserve raises interest rates, markets decline sharply\",\n",
    "    \"Google reports disappointing revenue, shares fall in after-hours trading\",\n",
    "    \"Strong job growth data boosts investor confidence in the economy\",\n",
    "    \"Oil prices plummet due to oversupply concerns and weak demand\",\n",
    "    \"Tech stocks rally on positive AI development news\",\n",
    "    \"Banking sector faces headwinds from regulatory changes\",\n",
    "    \"Cryptocurrency market shows signs of recovery after recent volatility\"\n",
    "]\n",
    "\n",
    "print(\"📰 Analisando sentimentos de notícias financeiras...\\n\")\n",
    "\n",
    "sentiment_results = []\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    # Análise de sentimento\n",
    "    sentiment_score = sentiment_analyzer.analyze_sentiment(text)\n",
    "    \n",
    "    # Classificar sentimento\n",
    "    if sentiment_score > 0.1:\n",
    "        sentiment_label = \"📈 POSITIVO\"\n",
    "        color = \"green\"\n",
    "    elif sentiment_score < -0.1:\n",
    "        sentiment_label = \"📉 NEGATIVO\"\n",
    "        color = \"red\"\n",
    "    else:\n",
    "        sentiment_label = \"➖ NEUTRO\"\n",
    "        color = \"gray\"\n",
    "    \n",
    "    sentiment_results.append({\n",
    "        'text': text,\n",
    "        'score': sentiment_score,\n",
    "        'label': sentiment_label\n",
    "    })\n",
    "    \n",
    "    print(f\"{i:2d}. {sentiment_label} ({sentiment_score:+.3f})\")\n",
    "    print(f\"    '{text}'\\n\")\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "sentiment_df = pd.DataFrame(sentiment_results)\n",
    "\n",
    "print(\"📊 Distribuição de Sentimentos:\")\n",
    "sentiment_counts = sentiment_df['label'].value_counts()\n",
    "for label, count in sentiment_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(sentiment_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuição de sentimentos\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histograma de scores\n",
    "axes[0].hist(sentiment_df['score'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Neutro')\n",
    "axes[0].set_xlabel('Score de Sentimento')\n",
    "axes[0].set_ylabel('Frequência')\n",
    "axes[0].set_title('📊 Distribuição dos Scores de Sentimento')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico de barras por categoria\n",
    "sentiment_counts.plot(kind='bar', ax=axes[1], color=['red', 'gray', 'green'], alpha=0.7)\n",
    "axes[1].set_xlabel('Categoria de Sentimento')\n",
    "axes[1].set_ylabel('Quantidade')\n",
    "axes[1].set_title('📊 Contagem por Categoria')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📈 Score médio: {sentiment_df['score'].mean():.3f}\")\n",
    "print(f\"📊 Desvio padrão: {sentiment_df['score'].std():.3f}\")\n",
    "print(f\"📉 Score mínimo: {sentiment_df['score'].min():.3f}\")\n",
    "print(f\"📈 Score máximo: {sentiment_df['score'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 🔗 Correlação Sentimento vs Preços\n",
    "\n",
    "Vamos simular a correlação entre sentimentos e movimentos de preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular série temporal de sentimentos\n",
    "print(\"🔗 Simulando correlação Sentimento vs Preços\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Pegar dados de um símbolo específico\n",
    "symbol = 'AAPL'\n",
    "symbol_data = df_enhanced[df_enhanced['symbol'] == symbol].copy().sort_values('date')\n",
    "\n",
    "# Simular sentimentos diários (correlacionados com retornos)\n",
    "np.random.seed(42)\n",
    "returns = symbol_data['close'].pct_change().fillna(0)\n",
    "\n",
    "# Criar sentimentos com alguma correlação com retornos + ruído\n",
    "sentiment_scores = []\n",
    "for i, ret in enumerate(returns):\n",
    "    # Sentimento baseado no retorno + ruído\n",
    "    base_sentiment = ret * 2  # Amplificar correlação\n",
    "    noise = np.random.normal(0, 0.1)  # Adicionar ruído\n",
    "    sentiment = base_sentiment + noise\n",
    "    \n",
    "    # Limitar entre -1 e 1\n",
    "    sentiment = np.clip(sentiment, -1, 1)\n",
    "    sentiment_scores.append(sentiment)\n",
    "\n",
    "symbol_data['sentiment'] = sentiment_scores\n",
    "symbol_data['returns'] = returns\n",
    "\n",
    "# Calcular correlação\n",
    "correlation = symbol_data['sentiment'].corr(symbol_data['returns'])\n",
    "print(f\"📊 Correlação Sentimento vs Retornos: {correlation:.3f}\")\n",
    "\n",
    "# Análise de lag (sentimento pode preceder movimento de preços)\n",
    "lag_correlations = []\n",
    "for lag in range(0, 6):\n",
    "    if lag == 0:\n",
    "        corr = symbol_data['sentiment'].corr(symbol_data['returns'])\n",
    "    else:\n",
    "        corr = symbol_data['sentiment'].corr(symbol_data['returns'].shift(-lag))\n",
    "    lag_correlations.append(corr)\n",
    "    print(f\"📈 Correlação com lag {lag}: {corr:.3f}\")\n",
    "\n",
    "# Visualizar correlações\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Série temporal de sentimentos e preços\n",
    "ax1 = axes[0, 0]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "dates = symbol_data['date'].iloc[-100:]  # Últimos 100 dias\n",
    "sentiment_recent = symbol_data['sentiment'].iloc[-100:]\n",
    "price_recent = symbol_data['close'].iloc[-100:]\n",
    "\n",
    "line1 = ax1.plot(dates, sentiment_recent, 'b-', alpha=0.7, label='Sentimento')\n",
    "line2 = ax2.plot(dates, price_recent, 'r-', alpha=0.7, label='Preço')\n",
    "\n",
    "ax1.set_xlabel('Data')\n",
    "ax1.set_ylabel('Score de Sentimento', color='b')\n",
    "ax2.set_ylabel('Preço ($)', color='r')\n",
    "ax1.set_title(f'📊 Sentimento vs Preço - {symbol}')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Scatter plot\n",
    "axes[0, 1].scatter(symbol_data['sentiment'], symbol_data['returns'], alpha=0.6)\n",
    "axes[0, 1].set_xlabel('Score de Sentimento')\n",
    "axes[0, 1].set_ylabel('Retorno Diário')\n",
    "axes[0, 1].set_title(f'🔗 Correlação: {correlation:.3f}')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlações com lag\n",
    "axes[1, 0].bar(range(len(lag_correlations)), lag_correlations, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Lag (dias)')\n",
    "axes[1, 0].set_ylabel('Correlação')\n",
    "axes[1, 0].set_title('📈 Correlação por Lag')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribuição de sentimentos\n",
    "axes[1, 1].hist(symbol_data['sentiment'], bins=20, alpha=0.7, color='green')\n",
    "axes[1, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Score de Sentimento')\n",
    "axes[1, 1].set_ylabel('Frequência')\n",
    "axes[1, 1].set_title('📊 Distribuição de Sentimentos')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 🔍 Interpretabilidade dos Modelos\n",
    "\n",
    "Vamos analisar quais features são mais importantes nos nossos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de importância das features\n",
    "print(\"🔍 Análise de Importância das Features\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Usar Random Forest para análise de importância\n",
    "rf_reg = regression_results['Random Forest']['model']\n",
    "rf_class = classification_results['Random Forest']['model']\n",
    "\n",
    "# Importância para regressão\n",
    "feature_importance_reg = rf_reg.feature_importances_\n",
    "feature_importance_class = rf_class.feature_importances_\n",
    "\n",
    "# Criar DataFrame com importâncias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names[:len(feature_importance_reg)],\n",
    "    'Importance_Regression': feature_importance_reg,\n",
    "    'Importance_Classification': feature_importance_class\n",
    "})\n",
    "\n",
    "# Ordenar por importância média\n",
    "importance_df['Importance_Mean'] = (importance_df['Importance_Regression'] + \n",
    "                                  importance_df['Importance_Classification']) / 2\n",
    "importance_df = importance_df.sort_values('Importance_Mean', ascending=False)\n",
    "\n",
    "# Top 15 features mais importantes\n",
    "top_features = importance_df.head(15)\n",
    "\n",
    "print(\"🏆 Top 15 Features Mais Importantes:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['Feature']:<25} | Reg: {row['Importance_Regression']:.4f} | Class: {row['Importance_Classification']:.4f}\")\n",
    "\n",
    "# Visualizar importâncias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Importância para regressão\n",
    "top_features.plot(x='Feature', y='Importance_Regression', kind='barh', \n",
    "                 ax=axes[0], color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('📊 Importância das Features - Regressão')\n",
    "axes[0].set_xlabel('Importância')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Importância para classificação\n",
    "top_features.plot(x='Feature', y='Importance_Classification', kind='barh', \n",
    "                 ax=axes[1], color='coral', alpha=0.7)\n",
    "axes[1].set_title('🎯 Importância das Features - Classificação')\n",
    "axes[1].set_xlabel('Importância')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ⚡ Validação Cruzada Temporal\n",
    "\n",
    "Vamos fazer uma validação mais robusta usando Time Series Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validação cruzada temporal\n",
    "print(\"⚡ Validação Cruzada Temporal\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Configurar Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Testar melhor modelo de cada tipo\n",
    "models_to_validate = {\n",
    "    'Best Regression': regression_results[best_model_name]['model'],\n",
    "    'Best Classification': classification_results[best_class_model_name]['model']\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for model_name, model in models_to_validate.items():\n",
    "    print(f\"\\n🔧 Validando {model_name}...\")\n",
    "    \n",
    "    if 'Regression' in model_name:\n",
    "        # Validação para regressão\n",
    "        scores = cross_val_score(model, X_train_scaled, y_price_train, \n",
    "                               cv=tscv, scoring='r2')\n",
    "        metric_name = 'R²'\n",
    "    else:\n",
    "        # Validação para classificação\n",
    "        scores = cross_val_score(model, X_train_scaled, y_dir_train, \n",
    "                               cv=tscv, scoring='accuracy')\n",
    "        metric_name = 'Accuracy'\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'scores': scores,\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'metric': metric_name\n",
    "    }\n",
    "    \n",
    "    print(f\"  📊 {metric_name} médio: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "    print(f\"  📈 Scores por fold: {[f'{s:.4f}' for s in scores]}\")\n",
    "\n",
    "# Visualizar resultados da validação cruzada\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, (model_name, results) in enumerate(cv_results.items()):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    \n",
    "    scores = results['scores']\n",
    "    folds = range(1, len(scores) + 1)\n",
    "    \n",
    "    plt.plot(folds, scores, 'o-', linewidth=2, markersize=8, alpha=0.7)\n",
    "    plt.axhline(y=results['mean'], color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Média: {results[\"mean\"]:.4f}')\n",
    "    \n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel(results['metric'])\n",
    "    plt.title(f'📊 Validação Cruzada - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(folds)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Validação cruzada concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 📊 Relatório Final e Insights\n",
    "\n",
    "Vamos gerar um relatório final com todos os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relatório final\n",
    "print(\"📊 RELATÓRIO FINAL - MACHINE LEARNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n📈 DADOS PROCESSADOS:\")\n",
    "print(f\"  • Total de amostras: {X_combined.shape[0]:,}\")\n",
    "print(f\"  • Features criadas: {X_combined.shape[1]}\")\n",
    "print(f\"  • Símbolos analisados: {len(df_enhanced['symbol'].unique())}\")\n",
    "print(f\"  • Período: {df_enhanced['date'].min().strftime('%Y-%m-%d')} até {df_enhanced['date'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n🤖 MODELOS DE REGRESSÃO:\")\n",
    "print(f\"  🏆 Melhor modelo: {best_model_name}\")\n",
    "print(f\"  📊 Test R²: {results_df.iloc[0]['Test R²']:.4f}\")\n",
    "print(f\"  📊 Test MSE: {results_df.iloc[0]['Test MSE']:.4f}\")\n",
    "print(f\"  ⚡ CV R² médio: {cv_results['Best Regression']['mean']:.4f} (±{cv_results['Best Regression']['std']:.4f})\")\n",
    "\n",
    "print(f\"\\n🎯 MODELOS DE CLASSIFICAÇÃO:\")\n",
    "print(f\"  🏆 Melhor modelo: {best_class_model_name}\")\n",
    "print(f\"  📊 Test Accuracy: {class_results_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "print(f\"  ⚡ CV Accuracy médio: {cv_results['Best Classification']['mean']:.4f} (±{cv_results['Best Classification']['std']:.4f})\")\n",
    "\n",
    "print(f\"\\n💭 ANÁLISE DE SENTIMENTOS:\")\n",
    "print(f\"  📊 Textos analisados: {len(sentiment_df)}\")\n",
    "print(f\"  📈 Score médio: {sentiment_df['score'].mean():.3f}\")\n",
    "print(f\"  🔗 Correlação simulada: {correlation:.3f}\")\n",
    "print(f\"  📊 Distribuição: {dict(sentiment_counts)}\")\n",
    "\n",
    "print(f\"\\n🔍 FEATURES MAIS IMPORTANTES:\")\n",
    "for i, (_, row) in enumerate(top_features.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Feature']} (Importância: {row['Importance_Mean']:.4f})\")\n",
    "\n",
    "print(f\"\\n🎯 INSIGHTS E RECOMENDAÇÕES:\")\n",
    "print(f\"  • Os modelos ensemble (Random Forest, Gradient Boosting) geralmente performam melhor\")\n",
    "print(f\"  • Features técnicas (médias móveis, volatilidade) são muito importantes\")\n",
    "print(f\"  • A validação cruzada temporal é essencial para dados financeiros\")\n",
    "print(f\"  • Sentimentos podem ser um indicador útil quando combinados com dados técnicos\")\n",
    "print(f\"  • É importante considerar o lag entre sentimentos e movimentos de preços\")\n",
    "\n",
    "print(f\"\\n🔄 PRÓXIMOS PASSOS:\")\n",
    "print(f\"  1. 📊 Implementar mais features de sentimento em tempo real\")\n",
    "print(f\"  2. 🔧 Otimizar hiperparâmetros dos modelos\")\n",
    "print(f\"  3. 📈 Criar ensemble de modelos\")\n",
    "print(f\"  4. 🎯 Implementar estratégias de trading baseadas nas predições\")\n",
    "print(f\"  5. 📊 Monitorar performance em produção\")\n",
    "\n",
    "print(f\"\\n✨ Análise de Machine Learning concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 💾 Salvamento dos Modelos\n",
    "\n",
    "Vamos salvar os melhores modelos para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos e resultados\n",
    "import joblib\n",
    "\n",
    "print(\"💾 Salvando modelos e resultados...\")\n",
    "\n",
    "# Criar diretório de modelos\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Salvar melhores modelos\n",
    "joblib.dump(best_model, f'{models_dir}/best_regression_model.pkl')\n",
    "joblib.dump(best_class_model, f'{models_dir}/best_classification_model.pkl')\n",
    "joblib.dump(scaler, f'{models_dir}/feature_scaler.pkl')\n",
    "\n",
    "print(f\"✅ Modelos salvos em: {models_dir}\")\n",
    "\n",
    "# Salvar resultados\n",
    "results_dir = '../data/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Salvar DataFrames de resultados\n",
    "results_df.to_csv(f'{results_dir}/regression_results.csv', index=False)\n",
    "class_results_df.to_csv(f'{results_dir}/classification_results.csv', index=False)\n",
    "importance_df.to_csv(f'{results_dir}/feature_importance.csv', index=False)\n",
    "sentiment_df.to_csv(f'{results_dir}/sentiment_analysis.csv', index=False)\n",
    "\n",
    "print(f\"✅ Resultados salvos em: {results_dir}\")\n",
    "\n",
    "# Salvar configurações\n",
    "config = {\n",
    "    'best_regression_model': best_model_name,\n",
    "    'best_classification_model': best_class_model_name,\n",
    "    'feature_names': feature_names,\n",
    "    'symbols_analyzed': list(df_enhanced['symbol'].unique()),\n",
    "    'date_range': {\n",
    "        'start': df_enhanced['date'].min().strftime('%Y-%m-%d'),\n",
    "        'end': df_enhanced['date'].max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'performance': {\n",
    "        'regression_r2': float(results_df.iloc[0]['Test R²']),\n",
    "        'classification_accuracy': float(class_results_df.iloc[0]['Test Accuracy']),\n",
    "        'sentiment_correlation': float(correlation)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{results_dir}/ml_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"✅ Configurações salvas em: {results_dir}/ml_config.json\")\n",
    "print(f\"\\n📁 Arquivos criados:\")\n",
    "print(f\"  • {models_dir}/best_regression_model.pkl\")\n",
    "print(f\"  • {models_dir}/best_classification_model.pkl\")\n",
    "print(f\"  • {models_dir}/feature_scaler.pkl\")\n",
    "print(f\"  • {results_dir}/regression_results.csv\")\n",
    "print(f\"  • {results_dir}/classification_results.csv\")\n",
    "print(f\"  • {results_dir}/feature_importance.csv\")\n",
    "print(f\"  • {results_dir}/sentiment_analysis.csv\")\n",
    "print(f\"  • {results_dir}/ml_config.json\")\n",
    "\n",
    "print(f\"\\n🎉 Notebook de Machine Learning concluído com sucesso!\")\n",
    "print(f\"📊 Execute o próximo notebook para visualizações interativas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Resumo do Notebook\n",
    "\n",
    "Este notebook demonstrou:\n",
    "\n",
    "### ✅ **Funcionalidades Implementadas:**\n",
    "- 🔧 **Engenharia de Features**: Indicadores técnicos, lags, rolling features\n",
    "- 🤖 **Modelos de Regressão**: Predição de preços futuros\n",
    "- 🎯 **Modelos de Classificação**: Predição da direção dos preços\n",
    "- 💭 **Análise de Sentimentos**: Processamento de textos financeiros\n",
    "- 🔗 **Correlação**: Sentimentos vs movimentos de preços\n",
    "- 🔍 **Interpretabilidade**: Análise de importância das features\n",
    "- ⚡ **Validação Robusta**: Time Series Cross-Validation\n",
    "- 💾 **Persistência**: Salvamento de modelos e resultados\n",
    "\n",
    "### 📊 **Modelos Testados:**\n",
    "- **Regressão**: Linear, Ridge, Lasso, Random Forest, Gradient Boosting, SVR\n",
    "- **Classificação**: Logistic Regression, Random Forest, Gradient Boosting, SVM, Naive Bayes\n",
    "- **Sentimentos**: TextBlob, VADER, análise financeira customizada\n",
    "\n",
    "### 🎯 **Próximos Passos:**\n",
    "1. Execute o notebook `04_interactive_dashboard.ipynb` para visualizações\n",
    "2. Implemente estratégias de trading baseadas nas predições\n",
    "3. Configure monitoramento em tempo real\n",
    "4. Otimize hiperparâmetros dos modelos\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido por:** Anny Caroline Sousa & Equipe Big Data Finance  \n",
    "**Projeto:** Sistema de Análise Financeira com Big Data  \n",
    "**Versão:** 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos e resultados\n",
    "import joblib\n",
    "\n",
    "print(\"💾 Salvando modelos e resultados...\")\n",
    "\n",
    "# Criar diretório de modelos\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Salvar melhores modelos\n",
    "joblib.dump(best_model, f'{models_dir}/best_regression_model.pkl')\n",
    "joblib.dump(best_class_model, f'{models_dir}/best_classification_model.pkl')\n",
    "joblib.dump(scaler, f'{models_dir}/feature_scaler.pkl')\n",
    "\n",
    "print(f\"✅ Modelos salvos em: {models_dir}\")\n",
    "\n",
    "# Salvar resultados\n",
    "results_dir = '../data/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Salvar DataFrames de resultados\n",
    "results_df.to_csv(f'{results_dir}/regression_results.csv', index=False)\n",
    "class_results_df.to_csv(f'{results_dir}/classification_results.csv', index=False)\n",
    "importance_df.to_csv(f'{results_dir}/feature_importance.csv', index=False)\n",
    "sentiment_df.to_csv(f'{results_dir}/sentiment_analysis.csv', index=False)\n",
    "\n",
    "print(f\"✅ Resultados salvos em: {results_dir}\")\n",
    "\n",
    "# Salvar configurações\n",
    "config = {\n",
    "    'best_regression_model': best_model_name,\n",
    "    'best_classification_model': best_class_model_name,\n",
    "    'feature_names': feature_names,\n",
    "    'symbols_analyzed': list(df_enhanced['symbol'].unique()),\n",
    "    'date_range': {\n",
    "        'start': df_enhanced['date'].min().strftime('%Y-%m-%d'),\n",
    "        'end': df_enhanced['date'].max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'performance': {\n",
    "        'regression_r2': float(results_df.iloc[0]['Test R²']),\n",
    "        'classification_accuracy': float(class_results_df.iloc[0]['Test Accuracy']),\n",
    "        'sentiment_correlation': float(correlation)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{results_dir}/ml_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"✅ Configurações salvas em: {results_dir}/ml_config.json\")\n",
    "print(f\"\\n📁 Arquivos criados:\")\n",
    "print(f\"  • {models_dir}/best_regression_model.pkl\")\n",
    "print(f\"  • {models_dir}/best_classification_model.pkl\")\n",
    "print(f\"  • {models_dir}/feature_scaler.pkl\")\n",
    "print(f\"  • {results_dir}/regression_results.csv\")\n",
    "print(f\"  • {results_dir}/classification_results.csv\")\n",
    "print(f\"  • {results_dir}/feature_importance.csv\")\n",
    "print(f\"  • {results_dir}/sentiment_analysis.csv\")\n",
    "print(f\"  • {results_dir}/ml_config.json\")\n",
    "\n",
    "print(f\"\\n🎉 Notebook de Machine Learning concluído com sucesso!\")\n",
    "print(f\"📊 Execute o próximo notebook para visualizações interativas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source