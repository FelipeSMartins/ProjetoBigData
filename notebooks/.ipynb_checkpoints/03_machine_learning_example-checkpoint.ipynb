{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Machine Learning e An√°lise de Sentimentos - Big Data Finance\n",
    "\n",
    "Este notebook demonstra como aplicar t√©cnicas de Machine Learning e an√°lise de sentimentos aos dados financeiros.\n",
    "\n",
    "## Objetivos\n",
    "- Engenharia de features para dados financeiros\n",
    "- Modelos preditivos para impacto de eventos\n",
    "- An√°lise de sentimento de not√≠cias\n",
    "- Avalia√ß√£o e persist√™ncia de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o de diret√≥rios\n",
    "data_dir = '../data/processed'\n",
    "results_dir = '../results'\n",
    "models_dir = os.path.join(results_dir, 'models')\n",
    "\n",
    "# Criar diret√≥rio de modelos se n√£o existir\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Diret√≥rios configurados:\")\n",
    "print(f\"  ‚Ä¢ Dados: {data_dir}\")\n",
    "print(f\"  ‚Ä¢ Resultados: {results_dir}\")\n",
    "print(f\"  ‚Ä¢ Modelos: {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados processados\n",
    "try:\n",
    "    # Dados de pre√ßos\n",
    "    prices_df = pd.read_csv(os.path.join(data_dir, 'prices_processed.csv'))\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "    \n",
    "    # Dados de eventos\n",
    "    events_df = pd.read_csv(os.path.join(data_dir, 'events_processed.csv'))\n",
    "    events_df['date'] = pd.to_datetime(events_df['date'])\n",
    "    \n",
    "    print(f\"‚úÖ Dados carregados com sucesso!\")\n",
    "    print(f\"üìà Pre√ßos: {len(prices_df)} registros\")\n",
    "    print(f\"üìÖ Eventos: {len(events_df)} registros\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "    print(\"üí° Execute primeiro os notebooks de coleta e processamento de dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Cria indicadores t√©cnicos para an√°lise\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # M√©dias m√≥veis\n",
    "    df['ma_7'] = df['close'].rolling(window=7).mean()\n",
    "    df['ma_21'] = df['close'].rolling(window=21).mean()\n",
    "    df['ma_50'] = df['close'].rolling(window=50).mean()\n",
    "    \n",
    "    # Volatilidade\n",
    "    df['volatility_7'] = df['close'].rolling(window=7).std()\n",
    "    df['volatility_21'] = df['close'].rolling(window=21).std()\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Retornos\n",
    "    df['return_1d'] = df['close'].pct_change()\n",
    "    df['return_7d'] = df['close'].pct_change(7)\n",
    "    df['return_21d'] = df['close'].pct_change(21)\n",
    "    \n",
    "    # Volume normalizado\n",
    "    df['volume_ma'] = df['volume'].rolling(window=21).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar engenharia de features (apenas se os dados foram carregados)\n",
    "if 'prices_df' in locals() and prices_df is not None:\n",
    "    prices_enhanced = create_technical_indicators(prices_df)\n",
    "    \n",
    "    print(\"üîß Features t√©cnicas criadas:\")\n",
    "    print(f\"  ‚Ä¢ M√©dias m√≥veis (7, 21, 50 dias)\")\n",
    "    print(f\"  ‚Ä¢ Volatilidade (7, 21 dias)\")\n",
    "    print(f\"  ‚Ä¢ RSI (14 dias)\")\n",
    "    print(f\"  ‚Ä¢ Retornos (1, 7, 21 dias)\")\n",
    "    print(f\"  ‚Ä¢ Volume normalizado\")\n",
    "    print(f\"\\nüìä Shape dos dados: {prices_enhanced.shape}\")\n",
    "else:\n",
    "    print(\"‚ùå Dados n√£o dispon√≠veis. Criando dados sint√©ticos para demonstra√ß√£o...\")\n",
    "    \n",
    "    # Criar dados sint√©ticos para demonstra√ß√£o\n",
    "    dates = pd.date_range('2023-01-01', '2024-01-01', freq='D')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    prices_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'symbol': 'DEMO',\n",
    "        'close': 100 + np.cumsum(np.random.randn(len(dates)) * 0.5),\n",
    "        'volume': np.random.randint(1000000, 10000000, len(dates)),\n",
    "        'high': 0,\n",
    "        'low': 0,\n",
    "        'open': 0\n",
    "    })\n",
    "    \n",
    "    # Ajustar high, low, open baseado no close\n",
    "    prices_df['high'] = prices_df['close'] * (1 + np.random.uniform(0, 0.02, len(prices_df)))\n",
    "    prices_df['low'] = prices_df['close'] * (1 - np.random.uniform(0, 0.02, len(prices_df)))\n",
    "    prices_df['open'] = prices_df['close'].shift(1).fillna(prices_df['close'].iloc[0])\n",
    "    \n",
    "    prices_enhanced = create_technical_indicators(prices_df)\n",
    "    \n",
    "    print(\"üîß Features t√©cnicas criadas com dados sint√©ticos:\")\n",
    "    print(f\"  ‚Ä¢ M√©dias m√≥veis (7, 21, 50 dias)\")\n",
    "    print(f\"  ‚Ä¢ Volatilidade (7, 21 dias)\")\n",
    "    print(f\"  ‚Ä¢ RSI (14 dias)\")\n",
    "    print(f\"  ‚Ä¢ Retornos (1, 7, 21 dias)\")\n",
    "    print(f\"  ‚Ä¢ Volume normalizado\")\n",
    "    print(f\"\\nüìä Shape dos dados: {prices_enhanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prepara√ß√£o do Dataset para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_dataset(prices_df, events_df, prediction_days=5):\n",
    "    \"\"\"\n",
    "    Prepara dataset para machine learning\n",
    "    \"\"\"\n",
    "    ml_data = []\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        event_date = event['date']\n",
    "        \n",
    "        # Per√≠odo antes do evento (features)\n",
    "        before_start = event_date - timedelta(days=30)\n",
    "        before_end = event_date - timedelta(days=1)\n",
    "        \n",
    "        # Per√≠odo ap√≥s o evento (target)\n",
    "        after_start = event_date + timedelta(days=1)\n",
    "        after_end = event_date + timedelta(days=prediction_days)\n",
    "        \n",
    "        # Dados antes do evento\n",
    "        before_data = prices_df[\n",
    "            (prices_df['date'] >= before_start) & \n",
    "            (prices_df['date'] <= before_end)\n",
    "        ].copy()\n",
    "        \n",
    "        # Dados ap√≥s o evento\n",
    "        after_data = prices_df[\n",
    "            (prices_df['date'] >= after_start) & \n",
    "            (prices_df['date'] <= after_end)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(before_data) >= 10 and len(after_data) >= prediction_days:\n",
    "            # Features do per√≠odo antes\n",
    "            features = {\n",
    "                'event_type': event['type'],\n",
    "                'event_severity': event.get('severity', 'medium'),\n",
    "                \n",
    "                # Pre√ßos\n",
    "                'price_mean_before': before_data['close'].mean(),\n",
    "                'price_std_before': before_data['close'].std(),\n",
    "                'price_trend_before': (before_data['close'].iloc[-1] - before_data['close'].iloc[0]) / before_data['close'].iloc[0],\n",
    "                \n",
    "                # Volume\n",
    "                'volume_mean_before': before_data['volume'].mean(),\n",
    "                'volume_std_before': before_data['volume'].std(),\n",
    "                \n",
    "                # Volatilidade\n",
    "                'volatility_before': before_data['close'].pct_change().std(),\n",
    "                \n",
    "                # RSI m√©dio\n",
    "                'rsi_mean_before': before_data['rsi'].mean() if 'rsi' in before_data.columns else 50,\n",
    "            }\n",
    "            \n",
    "            # Target: impacto ap√≥s o evento\n",
    "            price_before_event = before_data['close'].iloc[-1]\n",
    "            price_after_event = after_data['close'].iloc[-1]\n",
    "            impact = (price_after_event - price_before_event) / price_before_event\n",
    "            \n",
    "            features['impact'] = impact\n",
    "            features['event_date'] = event_date\n",
    "            \n",
    "            ml_data.append(features)\n",
    "    \n",
    "    return pd.DataFrame(ml_data)\n",
    "\n",
    "# Preparar dataset\n",
    "if 'events_df' in locals() and events_df is not None:\n",
    "    ml_dataset = prepare_ml_dataset(prices_enhanced, events_df)\n",
    "    \n",
    "    print(f\"üéØ Dataset ML preparado:\")\n",
    "    print(f\"  ‚Ä¢ {len(ml_dataset)} amostras\")\n",
    "    print(f\"  ‚Ä¢ {len(ml_dataset.columns)-2} features (excluindo target e data)\")\n",
    "    print(f\"\\nüìä Estat√≠sticas do impacto:\")\n",
    "    print(ml_dataset['impact'].describe())\n",
    "else:\n",
    "    print(\"‚ùå Dados de eventos n√£o dispon√≠veis. Criando dados sint√©ticos...\")\n",
    "    \n",
    "    # Criar eventos sint√©ticos\n",
    "    event_dates = pd.date_range('2023-02-01', '2023-12-01', freq='30D')\n",
    "    events_df = pd.DataFrame({\n",
    "        'date': event_dates,\n",
    "        'type': np.random.choice(['economic', 'political', 'market'], len(event_dates)),\n",
    "        'severity': np.random.choice(['low', 'medium', 'high'], len(event_dates))\n",
    "    })\n",
    "    \n",
    "    ml_dataset = prepare_ml_dataset(prices_enhanced, events_df)\n",
    "    \n",
    "    print(f\"üéØ Dataset ML preparado com dados sint√©ticos:\")\n",
    "    print(f\"  ‚Ä¢ {len(ml_dataset)} amostras\")\n",
    "    print(f\"  ‚Ä¢ {len(ml_dataset.columns)-2} features (excluindo target e data)\")\n",
    "    print(f\"\\nüìä Estat√≠sticas do impacto:\")\n",
    "    print(ml_dataset['impact'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Treinamento de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para treinamento\n",
    "if len(ml_dataset) > 0:\n",
    "    # Encoding de vari√°veis categ√≥ricas\n",
    "    le_type = LabelEncoder()\n",
    "    le_severity = LabelEncoder()\n",
    "    \n",
    "    ml_dataset['event_type_encoded'] = le_type.fit_transform(ml_dataset['event_type'])\n",
    "    ml_dataset['event_severity_encoded'] = le_severity.fit_transform(ml_dataset['event_severity'])\n",
    "    \n",
    "    # Features e target\n",
    "    feature_columns = [\n",
    "        'event_type_encoded', 'event_severity_encoded',\n",
    "        'price_mean_before', 'price_std_before', 'price_trend_before',\n",
    "        'volume_mean_before', 'volume_std_before',\n",
    "        'volatility_before', 'rsi_mean_before'\n",
    "    ]\n",
    "    \n",
    "    X = ml_dataset[feature_columns]\n",
    "    y = ml_dataset['impact']\n",
    "    \n",
    "    # Remover NaN\n",
    "    mask = ~(X.isna().any(axis=1) | y.isna())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"üìä Dados para treinamento:\")\n",
    "    print(f\"  ‚Ä¢ Amostras: {len(X)}\")\n",
    "    print(f\"  ‚Ä¢ Features: {len(feature_columns)}\")\n",
    "    \n",
    "    if len(X) >= 10:\n",
    "        # Split dos dados\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Normaliza√ß√£o\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        print(f\"‚úÖ Dados preparados para treinamento\")\n",
    "        print(f\"  ‚Ä¢ Treino: {len(X_train)} amostras\")\n",
    "        print(f\"  ‚Ä¢ Teste: {len(X_test)} amostras\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Dados insuficientes para treinamento (m√≠nimo 10 amostras)\")\nelse:\n    print(\"‚ùå Nenhum dado dispon√≠vel para treinamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento dos modelos\n",
    "if len(X) >= 10:\n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nü§ñ Treinando {name}...\")\n",
    "        \n",
    "        # Treinamento\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "        \n",
    "        # M√©tricas\n",
    "        train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "        test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"  üìä M√©tricas:\")\n",
    "        print(f\"    ‚Ä¢ R¬≤ Treino: {train_r2:.4f}\")\n",
    "        print(f\"    ‚Ä¢ R¬≤ Teste: {test_r2:.4f}\")\n",
    "        print(f\"    ‚Ä¢ MSE Teste: {test_mse:.6f}\")\n",
    "        print(f\"    ‚Ä¢ CV R¬≤ (m√©dia ¬± std): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Treinamento conclu√≠do!\")\nelse:\n    print(\"‚ö†Ô∏è Pulando treinamento - dados insuficientes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä An√°lise de Import√¢ncia das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de import√¢ncia das features\n",
    "if len(X) >= 10 and 'Random Forest' in results:\n",
    "    rf_model = results['Random Forest']['model']\n",
    "    \n",
    "    # Import√¢ncia das features\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"üîç Import√¢ncia das Features (Random Forest):\")\n",
    "    for _, row in feature_importance.iterrows():\n",
    "        print(f\"  ‚Ä¢ {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "    plt.title('Import√¢ncia das Features - Random Forest')\n",
    "    plt.xlabel('Import√¢ncia')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salvar gr√°fico\n",
    "    plt.savefig(os.path.join(results_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüíæ Gr√°fico salvo em: {results_dir}/feature_importance.png\")\nelse:\n    print(\"‚ö†Ô∏è An√°lise de import√¢ncia n√£o dispon√≠vel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Salvamento dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos e componentes\n",
    "if len(X) >= 10:\n",
    "    # Salvar melhor modelo (baseado em R¬≤ de teste)\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['test_r2'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    \n",
    "    # Salvar modelo\n",
    "    model_path = os.path.join(models_dir, 'best_impact_model.joblib')\n",
    "    joblib.dump(best_model, model_path)\n",
    "    \n",
    "    # Salvar scaler\n",
    "    scaler_path = os.path.join(models_dir, 'scaler.joblib')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    \n",
    "    # Salvar encoders\n",
    "    encoders_path = os.path.join(models_dir, 'encoders.joblib')\n",
    "    joblib.dump({\n",
    "        'event_type': le_type,\n",
    "        'event_severity': le_severity\n",
    "    }, encoders_path)\n",
    "    \n",
    "    # Salvar configura√ß√£o\n",
    "    config = {\n",
    "        'best_model': best_model_name,\n",
    "        'feature_columns': feature_columns,\n",
    "        'model_performance': {\n",
    "            name: {\n",
    "                'test_r2': float(metrics['test_r2']),\n",
    "                'test_mse': float(metrics['test_mse']),\n",
    "                'cv_mean': float(metrics['cv_mean']),\n",
    "                'cv_std': float(metrics['cv_std'])\n",
    "            }\n",
    "            for name, metrics in results.items()\n",
    "        },\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'n_samples': len(X)\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(models_dir, 'ml_config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Modelos salvos com sucesso!\")\n",
    "    print(f\"  ‚Ä¢ Melhor modelo: {best_model_name}\")\n",
    "    print(f\"  ‚Ä¢ R¬≤ de teste: {results[best_model_name]['test_r2']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Arquivos salvos em: {models_dir}\")\n",
    "    print(f\"    - best_impact_model.joblib\")\n",
    "    print(f\"    - scaler.joblib\")\n",
    "    print(f\"    - encoders.joblib\")\n",
    "    print(f\"    - ml_config.json\")\nelse:\n    print(\"‚ö†Ô∏è Nenhum modelo para salvar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclus√£o\n",
    "\n",
    "Este notebook demonstrou como aplicar t√©cnicas de Machine Learning para an√°lise de eventos financeiros:\n",
    "\n",
    "- **Modelos Preditivos**: Random Forest e Gradient Boosting\n",
    "- **An√°lise de Sentimento**: Processamento de texto\n",
    "- **Avalia√ß√£o de Performance**: M√©tricas e valida√ß√£o\n",
    "- **Persist√™ncia**: Salvamento de modelos e resultados\n",
    "\n",
    "Os modelos treinados podem ser utilizados para:\n",
    "- Prever impactos de novos eventos\n",
    "- Classificar severidade de eventos\n",
    "- Analisar sentimento de not√≠cias\n",
    "- Apoiar decis√µes de investimento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}