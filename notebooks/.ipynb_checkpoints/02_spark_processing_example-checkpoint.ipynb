{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Processamento com Apache Spark - Big Data Finance\n",
    "\n",
    "Este notebook demonstra como usar Apache Spark para processamento distribu√≠do de dados financeiros.\n",
    "\n",
    "## Objetivos\n",
    "- Configurar e inicializar Spark Session\n",
    "- Carregar dados financeiros no Spark\n",
    "- Realizar transforma√ß√µes distribu√≠das\n",
    "- Calcular indicadores t√©cnicos\n",
    "- Salvar resultados no HDFS\n",
    "- Demonstrar otimiza√ß√µes de performance\n",
    "\n",
    "**Autor:** Ana Luiza Pazze (Arquitetura e Infraestrutura) & Equipe Big Data Finance  \n",
    "**Gest√£o:** Fabio  \n",
    "**Processamento Spark:** Ana Luiza Pazze  \n",
    "**Data:** 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports realizados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Imports necess√°rios\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Imports do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Imports dos m√≥dulos do projeto\n",
    "from infrastructure.spark_manager import SparkManager\n",
    "from infrastructure.hdfs_manager import HDFSManager\n",
    "\n",
    "print(\"‚úÖ Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ‚öôÔ∏è Configura√ß√£o do Ambiente Spark\n",
    "\n",
    "Vamos configurar e inicializar o Spark para processamento distribu√≠do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Erro ao criar sess√£o Spark: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n",
      "\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1575)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m spark_manager = SparkManager(app_name=\u001b[33m\"\u001b[39m\u001b[33mBigDataFinance_Processing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Criar Spark Session otimizada\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m spark = \u001b[43mspark_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_spark_session\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdriver_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Spark Session criada com sucesso!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîß Spark Version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\Documents\\GitHub\\ProjetoBigData\\notebooks\\../src\\infrastructure\\spark_manager.py:80\u001b[39m, in \u001b[36mSparkManager.create_spark_session\u001b[39m\u001b[34m(self, master, executor_memory, driver_memory)\u001b[39m\n\u001b[32m     77\u001b[39m conf.set(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.shuffle.partitions\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m200\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m conf.set(\u001b[33m\"\u001b[39m\u001b[33mspark.serializer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33morg.apache.spark.serializer.KryoSerializer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28mself\u001b[39m.spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mself\u001b[39m.spark.sparkContext.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSess√£o Spark criada: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.app_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:207\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:300\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:429\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1627\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1621\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1623\u001b[39m     args_command +\\\n\u001b[32m   1624\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1626\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felipe.martins\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n"
     ]
    }
   ],
   "source": [
    "# Inicializar Spark Manager\n",
    "spark_manager = SparkManager(app_name=\"BigDataFinance_Processing\")\n",
    "\n",
    "# Criar Spark Session otimizada\n",
    "spark = spark_manager.create_spark_session(\n",
    "    master=\"local[*]\",\n",
    "    executor_memory=\"2g\",\n",
    "    driver_memory=\"2g\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark Session criada com sucesso!\")\n",
    "print(f\"üîß Spark Version: {spark.version}\")\n",
    "print(f\"üîß Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"üîß Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Configura√ß√µes do Spark\n",
    "print(\"\\n‚öôÔ∏è Configura√ß√µes do Spark:\")\n",
    "print(\"=\" * 40)\n",
    "configs = spark.sparkContext.getConf().getAll()\n",
    "for key, value in sorted(configs):\n",
    "    if 'spark.sql' in key or 'spark.executor' in key or 'spark.driver' in key:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üìä Carregamento de Dados\n",
    "\n",
    "Vamos carregar os dados financeiros coletados anteriormente no Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se existem dados para carregar\n",
    "data_dir = '../data/raw'\n",
    "stock_files = [f for f in os.listdir(data_dir) if f.startswith('stock_data_') and f.endswith('.csv')]\n",
    "\n",
    "if stock_files:\n",
    "    # Usar o arquivo mais recente\n",
    "    latest_file = sorted(stock_files)[-1]\n",
    "    stock_file_path = os.path.join(data_dir, latest_file)\n",
    "    print(f\"üìÅ Carregando dados de: {latest_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum arquivo de dados encontrado. Execute primeiro o notebook 01_data_collection_example.ipynb\")\n",
    "    # Criar dados de exemplo para demonstra√ß√£o\n",
    "    print(\"üîß Criando dados de exemplo...\")\n",
    "    \n",
    "    # Gerar dados sint√©ticos\n",
    "    dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='D')\n",
    "    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "    \n",
    "    data = []\n",
    "    for symbol in symbols:\n",
    "        base_price = np.random.uniform(100, 300)\n",
    "        for date in dates:\n",
    "            price_change = np.random.normal(0, 0.02)\n",
    "            base_price *= (1 + price_change)\n",
    "            \n",
    "            data.append({\n",
    "                'date': date.strftime('%Y-%m-%d'),\n",
    "                'symbol': symbol,\n",
    "                'open': base_price * np.random.uniform(0.98, 1.02),\n",
    "                'high': base_price * np.random.uniform(1.00, 1.05),\n",
    "                'low': base_price * np.random.uniform(0.95, 1.00),\n",
    "                'close': base_price,\n",
    "                'volume': np.random.randint(1000000, 10000000)\n",
    "            })\n",
    "    \n",
    "    # Salvar dados sint√©ticos\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    stock_file_path = os.path.join(data_dir, 'stock_data_synthetic.csv')\n",
    "    pd.DataFrame(data).to_csv(stock_file_path, index=False)\n",
    "    print(f\"‚úÖ Dados sint√©ticos criados: {stock_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados no Spark\n",
    "print(\"üìä Carregando dados no Spark...\")\n",
    "\n",
    "# Definir schema para otimizar carregamento\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"volume\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Carregar DataFrame\n",
    "df = spark.read.csv(\n",
    "    stock_file_path,\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Converter coluna de data\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Cache para melhor performance\n",
    "df.cache()\n",
    "\n",
    "print(f\"‚úÖ Dados carregados: {df.count():,} registros\")\n",
    "print(f\"üìã Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Visualizar primeiros registros\n",
    "print(\"\\nüìä Primeiros registros:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîÑ Transforma√ß√µes B√°sicas\n",
    "\n",
    "Vamos realizar transforma√ß√µes b√°sicas nos dados usando Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar DataFrame como tabela tempor√°ria\n",
    "df.createOrReplaceTempView(\"stock_data\")\n",
    "\n",
    "# Estat√≠sticas b√°sicas por s√≠mbolo\n",
    "print(\"üìä Estat√≠sticas por S√≠mbolo:\")\n",
    "stats_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        COUNT(*) as records,\n",
    "        MIN(date) as start_date,\n",
    "        MAX(date) as end_date,\n",
    "        ROUND(AVG(close), 2) as avg_price,\n",
    "        ROUND(MIN(close), 2) as min_price,\n",
    "        ROUND(MAX(close), 2) as max_price,\n",
    "        ROUND(AVG(volume), 0) as avg_volume\n",
    "    FROM stock_data \n",
    "    GROUP BY symbol\n",
    "    ORDER BY symbol\n",
    "\"\"\")\n",
    "\n",
    "stats_df.show()\n",
    "\n",
    "# Converter para Pandas para visualiza√ß√£o\n",
    "stats_pandas = stats_df.toPandas()\n",
    "print(f\"\\nüìà Resumo: {len(stats_pandas)} s√≠mbolos analisados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üìà C√°lculo de Indicadores T√©cnicos\n",
    "\n",
    "Vamos calcular indicadores t√©cnicos usando Window Functions do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir window specifications\n",
    "window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "window_7d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "window_20d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-19, 0)\n",
    "window_50d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-49, 0)\n",
    "\n",
    "print(\"üìä Calculando indicadores t√©cnicos...\")\n",
    "\n",
    "# Calcular indicadores\n",
    "df_indicators = df.withColumn(\n",
    "    \"daily_return\", \n",
    "    (col(\"close\") / lag(\"close\", 1).over(window_spec) - 1) * 100\n",
    ").withColumn(\n",
    "    \"sma_7\", \n",
    "    avg(\"close\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"sma_20\", \n",
    "    avg(\"close\").over(window_20d)\n",
    ").withColumn(\n",
    "    \"sma_50\", \n",
    "    avg(\"close\").over(window_50d)\n",
    ").withColumn(\n",
    "    \"volatility_7d\", \n",
    "    stddev(\"daily_return\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"price_change\", \n",
    "    col(\"close\") - col(\"open\")\n",
    ").withColumn(\n",
    "    \"price_range\", \n",
    "    col(\"high\") - col(\"low\")\n",
    ").withColumn(\n",
    "    \"volume_sma_20\", \n",
    "    avg(\"volume\").over(window_20d)\n",
    ")\n",
    "\n",
    "# Cache do resultado\n",
    "df_indicators.cache()\n",
    "\n",
    "print(\"‚úÖ Indicadores calculados!\")\n",
    "print(\"\\nüìä Exemplo de dados com indicadores:\")\n",
    "df_indicators.select(\n",
    "    \"date\", \"symbol\", \"close\", \"daily_return\", \n",
    "    \"sma_7\", \"sma_20\", \"volatility_7d\"\n",
    ").filter(\n",
    "    col(\"symbol\") == \"AAPL\"\n",
    ").orderBy(\n",
    "    desc(\"date\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 An√°lise de Sinais de Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar sinais de trading baseados em m√©dias m√≥veis\n",
    "print(\"üìà Gerando sinais de trading...\")\n",
    "\n",
    "df_signals = df_indicators.withColumn(\n",
    "    \"signal\",\n",
    "    when(col(\"sma_7\") > col(\"sma_20\"), \"BUY\")\n",
    "    .when(col(\"sma_7\") < col(\"sma_20\"), \"SELL\")\n",
    "    .otherwise(\"HOLD\")\n",
    ").withColumn(\n",
    "    \"trend\",\n",
    "    when(col(\"close\") > col(\"sma_50\"), \"UPTREND\")\n",
    "    .when(col(\"close\") < col(\"sma_50\"), \"DOWNTREND\")\n",
    "    .otherwise(\"SIDEWAYS\")\n",
    ").withColumn(\n",
    "    \"volatility_level\",\n",
    "    when(col(\"volatility_7d\") > 3.0, \"HIGH\")\n",
    "    .when(col(\"volatility_7d\") > 1.5, \"MEDIUM\")\n",
    "    .otherwise(\"LOW\")\n",
    ")\n",
    "\n",
    "# An√°lise de sinais por s√≠mbolo\n",
    "signals_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        signal,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY symbol), 2) as percentage\n",
    "    FROM (\n",
    "        SELECT symbol, \n",
    "               CASE \n",
    "                   WHEN sma_7 > sma_20 THEN 'BUY'\n",
    "                   WHEN sma_7 < sma_20 THEN 'SELL'\n",
    "                   ELSE 'HOLD'\n",
    "               END as signal\n",
    "        FROM stock_data_indicators\n",
    "        WHERE sma_7 IS NOT NULL AND sma_20 IS NOT NULL\n",
    "    )\n",
    "    GROUP BY symbol, signal\n",
    "    ORDER BY symbol, signal\n",
    "\"\"\")\n",
    "\n",
    "# Registrar nova tabela\n",
    "df_signals.createOrReplaceTempView(\"stock_data_indicators\")\n",
    "\n",
    "print(\"üìä Distribui√ß√£o de Sinais por S√≠mbolo:\")\n",
    "signals_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîç An√°lises Avan√ßadas com Spark SQL\n",
    "\n",
    "Vamos realizar an√°lises mais complexas usando Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de performance mensal\n",
    "print(\"üìÖ An√°lise de Performance Mensal:\")\n",
    "\n",
    "monthly_performance = spark.sql(\"\"\"\n",
    "    WITH monthly_data AS (\n",
    "        SELECT \n",
    "            symbol,\n",
    "            YEAR(date) as year,\n",
    "            MONTH(date) as month,\n",
    "            FIRST_VALUE(close) OVER (\n",
    "                PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                ORDER BY date\n",
    "            ) as month_open,\n",
    "            LAST_VALUE(close) OVER (\n",
    "                PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                ORDER BY date \n",
    "                ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "            ) as month_close,\n",
    "            AVG(volume) as avg_volume\n",
    "        FROM stock_data_indicators\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        symbol,\n",
    "        year,\n",
    "        month,\n",
    "        ROUND((month_close / month_open - 1) * 100, 2) as monthly_return,\n",
    "        ROUND(avg_volume, 0) as avg_volume\n",
    "    FROM monthly_data\n",
    "    ORDER BY symbol, year, month\n",
    "\"\"\")\n",
    "\n",
    "monthly_performance.show(20)\n",
    "\n",
    "# Estat√≠sticas de performance\n",
    "print(\"\\nüìä Estat√≠sticas de Performance:\")\n",
    "performance_stats = spark.sql(\"\"\"\n",
    "    WITH monthly_returns AS (\n",
    "        SELECT \n",
    "            symbol,\n",
    "            ROUND((month_close / month_open - 1) * 100, 2) as monthly_return\n",
    "        FROM (\n",
    "            SELECT DISTINCT\n",
    "                symbol,\n",
    "                YEAR(date) as year,\n",
    "                MONTH(date) as month,\n",
    "                FIRST_VALUE(close) OVER (\n",
    "                    PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                    ORDER BY date\n",
    "                ) as month_open,\n",
    "                LAST_VALUE(close) OVER (\n",
    "                    PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "                ) as month_close\n",
    "            FROM stock_data_indicators\n",
    "        )\n",
    "    )\n",
    "    SELECT \n",
    "        symbol,\n",
    "        ROUND(AVG(monthly_return), 2) as avg_monthly_return,\n",
    "        ROUND(STDDEV(monthly_return), 2) as volatility,\n",
    "        ROUND(MIN(monthly_return), 2) as worst_month,\n",
    "        ROUND(MAX(monthly_return), 2) as best_month,\n",
    "        COUNT(*) as months_analyzed\n",
    "    FROM monthly_returns\n",
    "    GROUP BY symbol\n",
    "    ORDER BY avg_monthly_return DESC\n",
    "\"\"\")\n",
    "\n",
    "performance_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 An√°lise de Correla√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular correla√ß√µes entre ativos\n",
    "print(\"üîó An√°lise de Correla√ß√µes entre Ativos:\")\n",
    "\n",
    "# Pivot dos retornos di√°rios\n",
    "returns_pivot = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        MAX(CASE WHEN symbol = 'AAPL' THEN daily_return END) as AAPL,\n",
    "        MAX(CASE WHEN symbol = 'GOOGL' THEN daily_return END) as GOOGL,\n",
    "        MAX(CASE WHEN symbol = 'MSFT' THEN daily_return END) as MSFT,\n",
    "        MAX(CASE WHEN symbol = 'TSLA' THEN daily_return END) as TSLA,\n",
    "        MAX(CASE WHEN symbol = 'AMZN' THEN daily_return END) as AMZN\n",
    "    FROM stock_data_indicators\n",
    "    WHERE daily_return IS NOT NULL\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "returns_pivot.cache()\n",
    "print(\"üìä Matriz de retornos criada\")\n",
    "returns_pivot.show(10)\n",
    "\n",
    "# Converter para Pandas para calcular correla√ß√µes\n",
    "returns_pandas = returns_pivot.toPandas().set_index('date')\n",
    "correlation_matrix = returns_pandas.corr()\n",
    "\n",
    "print(\"\\nüîó Matriz de Correla√ß√£o:\")\n",
    "print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üíæ Salvamento no HDFS\n",
    "\n",
    "Vamos salvar os dados processados no HDFS para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar HDFS Manager (simulado para ambiente local)\n",
    "print(\"üíæ Preparando salvamento dos dados processados...\")\n",
    "\n",
    "# Criar diret√≥rio de sa√≠da\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Salvar dados com indicadores\n",
    "print(\"üìä Salvando dados com indicadores t√©cnicos...\")\n",
    "indicators_path = f\"{output_dir}/stock_indicators\"\n",
    "\n",
    "# Salvar como Parquet (formato otimizado)\n",
    "df_signals.coalesce(1).write.mode(\"overwrite\").parquet(indicators_path)\n",
    "print(f\"‚úÖ Dados salvos em: {indicators_path}\")\n",
    "\n",
    "# Salvar estat√≠sticas de performance\n",
    "print(\"üìà Salvando estat√≠sticas de performance...\")\n",
    "performance_path = f\"{output_dir}/performance_stats\"\n",
    "performance_stats.coalesce(1).write.mode(\"overwrite\").parquet(performance_path)\n",
    "print(f\"‚úÖ Estat√≠sticas salvas em: {performance_path}\")\n",
    "\n",
    "# Salvar dados mensais\n",
    "print(\"üìÖ Salvando dados de performance mensal...\")\n",
    "monthly_path = f\"{output_dir}/monthly_performance\"\n",
    "monthly_performance.coalesce(1).write.mode(\"overwrite\").parquet(monthly_path)\n",
    "print(f\"‚úÖ Dados mensais salvos em: {monthly_path}\")\n",
    "\n",
    "# Salvar como CSV tamb√©m para compatibilidade\n",
    "print(\"üìÑ Salvando vers√µes CSV...\")\n",
    "df_signals.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/stock_indicators_csv\")\n",
    "performance_stats.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/performance_stats_csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Todos os dados processados foram salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Visualiza√ß√µes dos Resultados\n",
    "\n",
    "Vamos criar algumas visualiza√ß√µes dos dados processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter dados para visualiza√ß√£o\n",
    "performance_pandas = performance_stats.toPandas()\n",
    "monthly_pandas = monthly_performance.toPandas()\n",
    "\n",
    "# Gr√°fico de performance m√©dia mensal\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "performance_pandas.set_index('symbol')['avg_monthly_return'].plot(kind='bar', color='skyblue')\n",
    "plt.title('üìà Retorno M√©dio Mensal por Ativo', fontweight='bold')\n",
    "plt.ylabel('Retorno (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico de volatilidade\n",
    "plt.subplot(1, 2, 2)\n",
    "performance_pandas.set_index('symbol')['volatility'].plot(kind='bar', color='coral')\n",
    "plt.title('üìä Volatilidade Mensal por Ativo', fontweight='bold')\n",
    "plt.ylabel('Volatilidade (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap de correla√ß√µes\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True, \n",
    "            linewidths=0.5,\n",
    "            fmt='.3f')\n",
    "plt.title('üîó Matriz de Correla√ß√£o dos Retornos (Spark Processing)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ‚ö° Otimiza√ß√µes de Performance\n",
    "\n",
    "Vamos demonstrar algumas otimiza√ß√µes importantes do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de performance do Spark\n",
    "print(\"‚ö° An√°lise de Performance do Spark\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Informa√ß√µes sobre cache\n",
    "print(\"üíæ DataFrames em Cache:\")\n",
    "cached_tables = spark.catalog.listTables()\n",
    "for table in cached_tables:\n",
    "    if table.isTemporary:\n",
    "        print(f\"  - {table.name}\")\n",
    "\n",
    "# Estat√≠sticas do Spark Context\n",
    "sc = spark.sparkContext\n",
    "print(f\"\\nüìä Estat√≠sticas do Spark Context:\")\n",
    "print(f\"  - Application ID: {sc.applicationId}\")\n",
    "print(f\"  - Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"  - Status: {sc.statusTracker().getExecutorInfos()}\")\n",
    "\n",
    "# Exemplo de particionamento otimizado\n",
    "print(\"\\nüîß Otimiza√ß√£o de Particionamento:\")\n",
    "print(f\"Parti√ß√µes atuais do DataFrame: {df_signals.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Reparticionamento por s√≠mbolo para otimizar opera√ß√µes por grupo\n",
    "df_optimized = df_signals.repartition(col(\"symbol\"))\n",
    "print(f\"Parti√ß√µes ap√≥s reparticionamento: {df_optimized.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Exemplo de broadcast join (simulado)\n",
    "print(\"\\nüì° Exemplo de Broadcast Join:\")\n",
    "# Criar pequena tabela de metadados\n",
    "metadata = spark.createDataFrame([\n",
    "    (\"AAPL\", \"Technology\", \"Apple Inc.\"),\n",
    "    (\"GOOGL\", \"Technology\", \"Alphabet Inc.\"),\n",
    "    (\"MSFT\", \"Technology\", \"Microsoft Corp.\"),\n",
    "    (\"TSLA\", \"Automotive\", \"Tesla Inc.\"),\n",
    "    (\"AMZN\", \"E-commerce\", \"Amazon.com Inc.\")\n",
    "], [\"symbol\", \"sector\", \"company_name\"])\n",
    "\n",
    "# Broadcast da tabela pequena\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_with_metadata = df_signals.join(\n",
    "    broadcast(metadata), \n",
    "    \"symbol\", \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Broadcast join configurado para otimizar performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üîç Monitoramento e Debugging\n",
    "\n",
    "Vamos ver como monitorar jobs do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes sobre jobs executados\n",
    "print(\"üîç Informa√ß√µes de Jobs do Spark\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Status tracker\n",
    "status_tracker = sc.statusTracker()\n",
    "\n",
    "# Informa√ß√µes dos executors\n",
    "executor_infos = status_tracker.getExecutorInfos()\n",
    "print(f\"üìä N√∫mero de Executors: {len(executor_infos)}\")\n",
    "\n",
    "for executor in executor_infos:\n",
    "    print(f\"\\nüîß Executor {executor.executorId}:\")\n",
    "    print(f\"  - Host: {executor.host}\")\n",
    "    print(f\"  - Cores: {executor.totalCores}\")\n",
    "    print(f\"  - Mem√≥ria M√°xima: {executor.maxMemory / (1024**3):.2f} GB\")\n",
    "    print(f\"  - Tasks Ativas: {executor.activeTasks}\")\n",
    "    print(f\"  - Tasks Completadas: {executor.completedTasks}\")\n",
    "\n",
    "# URL da Spark UI\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "if spark_ui_url:\n",
    "    print(f\"\\nüåê Spark UI dispon√≠vel em: {spark_ui_url}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Spark UI n√£o dispon√≠vel (modo local)\")\n",
    "\n",
    "# Exemplo de explain plan\n",
    "print(\"\\nüìã Plano de Execu√ß√£o (Explain):\")\n",
    "print(\"=\" * 40)\n",
    "df_signals.filter(col(\"symbol\") == \"AAPL\").select(\"date\", \"close\", \"sma_20\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üßπ Limpeza e Finaliza√ß√£o\n",
    "\n",
    "Vamos limpar recursos e finalizar a sess√£o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar cache\n",
    "print(\"üßπ Limpando cache...\")\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Unpersist DataFrames\n",
    "df.unpersist()\n",
    "df_indicators.unpersist()\n",
    "returns_pivot.unpersist()\n",
    "\n",
    "print(\"‚úÖ Cache limpo\")\n",
    "\n",
    "# Estat√≠sticas finais\n",
    "print(\"\\nüìä RESUMO DO PROCESSAMENTO SPARK\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"üìà Registros processados: {df.count():,}\")\n",
    "print(f\"üîß Indicadores calculados: 8 (SMA, volatilidade, sinais, etc.)\")\n",
    "print(f\"üíæ Arquivos salvos: 6 (Parquet + CSV)\")\n",
    "print(f\"‚ö° Executors utilizados: {len(executor_infos)}\")\n",
    "print(f\"üéØ Performance: Otimizada com cache e particionamento\")\n",
    "\n",
    "print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"1. üìä An√°lise estat√≠stica avan√ßada\")\n",
    "print(\"2. ü§ñ Aplicar modelos de ML\")\n",
    "print(\"3. üí≠ An√°lise de sentimentos\")\n",
    "print(\"4. üìà Dashboards interativos\")\n",
    "print(\"5. üîÑ Pipeline automatizado\")\n",
    "\n",
    "print(\"\\n‚ú® Processamento Spark conclu√≠do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizar Spark Session\n",
    "print(\"üîö Finalizando Spark Session...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark Session finalizada\")\n",
    "\n",
    "print(\"\\nüéâ Notebook conclu√≠do com sucesso!\")\n",
    "print(\"üìÅ Dados processados dispon√≠veis em: ../data/processed/\")\n",
    "print(\"üîÑ Execute o pr√≥ximo notebook para an√°lises de ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Refer√™ncias e Links √öteis\n",
    "\n",
    "- **Apache Spark**: [spark.apache.org](https://spark.apache.org/)\n",
    "- **PySpark Documentation**: [spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)\n",
    "- **Spark SQL Guide**: [spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- **Performance Tuning**: [spark.apache.org/docs/latest/tuning.html](https://spark.apache.org/docs/latest/tuning.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido pela Equipe Big Data Finance**  \n",
    "**Notebook:** 02_spark_processing_example.ipynb  \n",
    "**Vers√£o:** 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}