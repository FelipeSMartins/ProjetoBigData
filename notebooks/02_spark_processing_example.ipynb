{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Processamento com Apache Spark - Big Data Finance\n",
    "\n",
    "Este notebook demonstra como usar Apache Spark para processamento distribuÃ­do de dados financeiros.\n",
    "\n",
    "## Objetivos\n",
    "- Configurar e inicializar Spark Session\n",
    "- Carregar dados financeiros no Spark\n",
    "- Realizar transformaÃ§Ãµes distribuÃ­das\n",
    "- Calcular indicadores tÃ©cnicos\n",
    "- Salvar resultados no HDFS\n",
    "- Demonstrar otimizaÃ§Ãµes de performance\n",
    "\n",
    "**Autor:** Ana Luiza Pazze (Arquitetura e Infraestrutura) & Equipe Big Data Finance  \n",
    "**GestÃ£o:** Fabio  \n",
    "**Processamento Spark:** Ana Luiza Pazze  \n",
    "**Data:** 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessÃ¡rios\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Imports do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Imports dos mÃ³dulos do projeto\n",
    "from infrastructure.spark_manager import SparkManager\n",
    "from infrastructure.hdfs_manager import HDFSManager\n",
    "\n",
    "print(\"âœ… Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. âš™ï¸ ConfiguraÃ§Ã£o do Ambiente Spark\n",
    "\n",
    "Vamos configurar e inicializar o Spark para processamento distribuÃ­do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Spark Manager\n",
    "spark_manager = SparkManager()\n",
    "\n",
    "# Criar Spark Session otimizada\n",
    "spark = spark_manager.create_spark_session(\n",
    "    app_name=\"BigDataFinance_Processing\",\n",
    "    executor_memory=\"2g\",\n",
    "    executor_cores=2,\n",
    "    num_executors=2\n",
    ")\n",
    "\n",
    "print(\"âœ… Spark Session criada com sucesso!\")\n",
    "print(f\"ðŸ”§ Spark Version: {spark.version}\")\n",
    "print(f\"ðŸ”§ Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"ðŸ”§ Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# ConfiguraÃ§Ãµes do Spark\n",
    "print(\"\\nâš™ï¸ ConfiguraÃ§Ãµes do Spark:\")\n",
    "print(\"=\" * 40)\n",
    "configs = spark.sparkContext.getConf().getAll()\n",
    "for key, value in sorted(configs):\n",
    "    if 'spark.sql' in key or 'spark.executor' in key or 'spark.driver' in key:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸ“Š Carregamento de Dados\n",
    "\n",
    "Vamos carregar os dados financeiros coletados anteriormente no Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se existem dados para carregar\n",
    "data_dir = '../data/raw'\n",
    "stock_files = [f for f in os.listdir(data_dir) if f.startswith('stock_data_') and f.endswith('.csv')]\n",
    "\n",
    "if stock_files:\n",
    "    # Usar o arquivo mais recente\n",
    "    latest_file = sorted(stock_files)[-1]\n",
    "    stock_file_path = os.path.join(data_dir, latest_file)\n",
    "    print(f\"ðŸ“ Carregando dados de: {latest_file}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Nenhum arquivo de dados encontrado. Execute primeiro o notebook 01_data_collection_example.ipynb\")\n",
    "    # Criar dados de exemplo para demonstraÃ§Ã£o\n",
    "    print(\"ðŸ”§ Criando dados de exemplo...\")\n",
    "    \n",
    "    # Gerar dados sintÃ©ticos\n",
    "    dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='D')\n",
    "    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "    \n",
    "    data = []\n",
    "    for symbol in symbols:\n",
    "        base_price = np.random.uniform(100, 300)\n",
    "        for date in dates:\n",
    "            price_change = np.random.normal(0, 0.02)\n",
    "            base_price *= (1 + price_change)\n",
    "            \n",
    "            data.append({\n",
    "                'date': date.strftime('%Y-%m-%d'),\n",
    "                'symbol': symbol,\n",
    "                'open': base_price * np.random.uniform(0.98, 1.02),\n",
    "                'high': base_price * np.random.uniform(1.00, 1.05),\n",
    "                'low': base_price * np.random.uniform(0.95, 1.00),\n",
    "                'close': base_price,\n",
    "                'volume': np.random.randint(1000000, 10000000)\n",
    "            })\n",
    "    \n",
    "    # Salvar dados sintÃ©ticos\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    stock_file_path = os.path.join(data_dir, 'stock_data_synthetic.csv')\n",
    "    pd.DataFrame(data).to_csv(stock_file_path, index=False)\n",
    "    print(f\"âœ… Dados sintÃ©ticos criados: {stock_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados no Spark\n",
    "print(\"ðŸ“Š Carregando dados no Spark...\")\n",
    "\n",
    "# Definir schema para otimizar carregamento\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"volume\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Carregar DataFrame\n",
    "df = spark.read.csv(\n",
    "    stock_file_path,\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Converter coluna de data\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Cache para melhor performance\n",
    "df.cache()\n",
    "\n",
    "print(f\"âœ… Dados carregados: {df.count():,} registros\")\n",
    "print(f\"ðŸ“‹ Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Visualizar primeiros registros\n",
    "print(\"\\nðŸ“Š Primeiros registros:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸ”„ TransformaÃ§Ãµes BÃ¡sicas\n",
    "\n",
    "Vamos realizar transformaÃ§Ãµes bÃ¡sicas nos dados usando Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar DataFrame como tabela temporÃ¡ria\n",
    "df.createOrReplaceTempView(\"stock_data\")\n",
    "\n",
    "# EstatÃ­sticas bÃ¡sicas por sÃ­mbolo\n",
    "print(\"ðŸ“Š EstatÃ­sticas por SÃ­mbolo:\")\n",
    "stats_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        COUNT(*) as records,\n",
    "        MIN(date) as start_date,\n",
    "        MAX(date) as end_date,\n",
    "        ROUND(AVG(close), 2) as avg_price,\n",
    "        ROUND(MIN(close), 2) as min_price,\n",
    "        ROUND(MAX(close), 2) as max_price,\n",
    "        ROUND(AVG(volume), 0) as avg_volume\n",
    "    FROM stock_data \n",
    "    GROUP BY symbol\n",
    "    ORDER BY symbol\n",
    "\"\"\")\n",
    "\n",
    "stats_df.show()\n",
    "\n",
    "# Converter para Pandas para visualizaÃ§Ã£o\n",
    "stats_pandas = stats_df.toPandas()\n",
    "print(f\"\\nðŸ“ˆ Resumo: {len(stats_pandas)} sÃ­mbolos analisados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸ“ˆ CÃ¡lculo de Indicadores TÃ©cnicos\n",
    "\n",
    "Vamos calcular indicadores tÃ©cnicos usando Window Functions do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir window specifications\n",
    "window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "window_7d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "window_20d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-19, 0)\n",
    "window_50d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-49, 0)\n",
    "\n",
    "print(\"ðŸ“Š Calculando indicadores tÃ©cnicos...\")\n",
    "\n",
    "# Calcular indicadores\n",
    "df_indicators = df.withColumn(\n",
    "    \"daily_return\", \n",
    "    (col(\"close\") / lag(\"close\", 1).over(window_spec) - 1) * 100\n",
    ").withColumn(\n",
    "    \"sma_7\", \n",
    "    avg(\"close\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"sma_20\", \n",
    "    avg(\"close\").over(window_20d)\n",
    ").withColumn(\n",
    "    \"sma_50\", \n",
    "    avg(\"close\").over(window_50d)\n",
    ").withColumn(\n",
    "    \"volatility_7d\", \n",
    "    stddev(\"daily_return\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"price_change\", \n",
    "    col(\"close\") - col(\"open\")\n",
    ").withColumn(\n",
    "    \"price_range\", \n",
    "    col(\"high\") - col(\"low\")\n",
    ").withColumn(\n",
    "    \"volume_sma_20\", \n",
    "    avg(\"volume\").over(window_20d)\n",
    ")\n",
    "\n",
    "# Cache do resultado\n",
    "df_indicators.cache()\n",
    "\n",
    "print(\"âœ… Indicadores calculados!\")\n",
    "print(\"\\nðŸ“Š Exemplo de dados com indicadores:\")\n",
    "df_indicators.select(\n",
    "    \"date\", \"symbol\", \"close\", \"daily_return\", \n",
    "    \"sma_7\", \"sma_20\", \"volatility_7d\"\n",
    ").filter(\n",
    "    col(\"symbol\") == \"AAPL\"\n",
    ").orderBy(\n",
    "    desc(\"date\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 AnÃ¡lise de Sinais de Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar sinais de trading baseados em mÃ©dias mÃ³veis\n",
    "print(\"ðŸ“ˆ Gerando sinais de trading...\")\n",
    "\n",
    "df_signals = df_indicators.withColumn(\n",
    "    \"signal\",\n",
    "    when(col(\"sma_7\") > col(\"sma_20\"), \"BUY\")\n",
    "    .when(col(\"sma_7\") < col(\"sma_20\"), \"SELL\")\n",
    "    .otherwise(\"HOLD\")\n",
    ").withColumn(\n",
    "    \"trend\",\n",
    "    when(col(\"close\") > col(\"sma_50\"), \"UPTREND\")\n",
    "    .when(col(\"close\") < col(\"sma_50\"), \"DOWNTREND\")\n",
    "    .otherwise(\"SIDEWAYS\")\n",
    ").withColumn(\n",
    "    \"volatility_level\",\n",
    "    when(col(\"volatility_7d\") > 3.0, \"HIGH\")\n",
    "    .when(col(\"volatility_7d\") > 1.5, \"MEDIUM\")\n",
    "    .otherwise(\"LOW\")\n",
    ")\n",
    "\n",
    "# AnÃ¡lise de sinais por sÃ­mbolo\n",
    "signals_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        signal,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY symbol), 2) as percentage\n",
    "    FROM (\n",
    "        SELECT symbol, \n",
    "               CASE \n",
    "                   WHEN sma_7 > sma_20 THEN 'BUY'\n",
    "                   WHEN sma_7 < sma_20 THEN 'SELL'\n",
    "                   ELSE 'HOLD'\n",
    "               END as signal\n",
    "        FROM stock_data_indicators\n",
    "        WHERE sma_7 IS NOT NULL AND sma_20 IS NOT NULL\n",
    "    )\n",
    "    GROUP BY symbol, signal\n",
    "    ORDER BY symbol, signal\n",
    "\"\"\")\n",
    "\n",
    "# Registrar nova tabela\n",
    "df_signals.createOrReplaceTempView(\"stock_data_indicators\")\n",
    "\n",
    "print(\"ðŸ“Š DistribuiÃ§Ã£o de Sinais por SÃ­mbolo:\")\n",
    "signals_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ” AnÃ¡lises AvanÃ§adas com Spark SQL\n",
    "\n",
    "Vamos realizar anÃ¡lises mais complexas usando Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lise de performance mensal\n",
    "print(\"ðŸ“… AnÃ¡lise de Performance Mensal:\")\n",
    "\n",
    "monthly_performance = spark.sql(\"\"\"\n",
    "    WITH monthly_data AS (\n",
    "        SELECT \n",
    "            symbol,\n",
    "            YEAR(date) as year,\n",
    "            MONTH(date) as month,\n",
    "            FIRST_VALUE(close) OVER (\n",
    "                PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                ORDER BY date\n",
    "            ) as month_open,\n",
    "            LAST_VALUE(close) OVER (\n",
    "                PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                ORDER BY date \n",
    "                ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "            ) as month_close,\n",
    "            AVG(volume) as avg_volume\n",
    "        FROM stock_data_indicators\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        symbol,\n",
    "        year,\n",
    "        month,\n",
    "        ROUND((month_close / month_open - 1) * 100, 2) as monthly_return,\n",
    "        ROUND(avg_volume, 0) as avg_volume\n",
    "    FROM monthly_data\n",
    "    ORDER BY symbol, year, month\n",
    "\"\"\")\n",
    "\n",
    "monthly_performance.show(20)\n",
    "\n",
    "# EstatÃ­sticas de performance\n",
    "print(\"\\nðŸ“Š EstatÃ­sticas de Performance:\")\n",
    "performance_stats = spark.sql(\"\"\"\n",
    "    WITH monthly_returns AS (\n",
    "        SELECT \n",
    "            symbol,\n",
    "            ROUND((month_close / month_open - 1) * 100, 2) as monthly_return\n",
    "        FROM (\n",
    "            SELECT DISTINCT\n",
    "                symbol,\n",
    "                YEAR(date) as year,\n",
    "                MONTH(date) as month,\n",
    "                FIRST_VALUE(close) OVER (\n",
    "                    PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                    ORDER BY date\n",
    "                ) as month_open,\n",
    "                LAST_VALUE(close) OVER (\n",
    "                    PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "                ) as month_close\n",
    "            FROM stock_data_indicators\n",
    "        )\n",
    "    )\n",
    "    SELECT \n",
    "        symbol,\n",
    "        ROUND(AVG(monthly_return), 2) as avg_monthly_return,\n",
    "        ROUND(STDDEV(monthly_return), 2) as volatility,\n",
    "        ROUND(MIN(monthly_return), 2) as worst_month,\n",
    "        ROUND(MAX(monthly_return), 2) as best_month,\n",
    "        COUNT(*) as months_analyzed\n",
    "    FROM monthly_returns\n",
    "    GROUP BY symbol\n",
    "    ORDER BY avg_monthly_return DESC\n",
    "\"\"\")\n",
    "\n",
    "performance_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 AnÃ¡lise de CorrelaÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular correlaÃ§Ãµes entre ativos\n",
    "print(\"ðŸ”— AnÃ¡lise de CorrelaÃ§Ãµes entre Ativos:\")\n",
    "\n",
    "# Pivot dos retornos diÃ¡rios\n",
    "returns_pivot = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        MAX(CASE WHEN symbol = 'AAPL' THEN daily_return END) as AAPL,\n",
    "        MAX(CASE WHEN symbol = 'GOOGL' THEN daily_return END) as GOOGL,\n",
    "        MAX(CASE WHEN symbol = 'MSFT' THEN daily_return END) as MSFT,\n",
    "        MAX(CASE WHEN symbol = 'TSLA' THEN daily_return END) as TSLA,\n",
    "        MAX(CASE WHEN symbol = 'AMZN' THEN daily_return END) as AMZN\n",
    "    FROM stock_data_indicators\n",
    "    WHERE daily_return IS NOT NULL\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "returns_pivot.cache()\n",
    "print(\"ðŸ“Š Matriz de retornos criada\")\n",
    "returns_pivot.show(10)\n",
    "\n",
    "# Converter para Pandas para calcular correlaÃ§Ãµes\n",
    "returns_pandas = returns_pivot.toPandas().set_index('date')\n",
    "correlation_matrix = returns_pandas.corr()\n",
    "\n",
    "print(\"\\nðŸ”— Matriz de CorrelaÃ§Ã£o:\")\n",
    "print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ðŸ’¾ Salvamento no HDFS\n",
    "\n",
    "Vamos salvar os dados processados no HDFS para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar HDFS Manager (simulado para ambiente local)\n",
    "print(\"ðŸ’¾ Preparando salvamento dos dados processados...\")\n",
    "\n",
    "# Criar diretÃ³rio de saÃ­da\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Salvar dados com indicadores\n",
    "print(\"ðŸ“Š Salvando dados com indicadores tÃ©cnicos...\")\n",
    "indicators_path = f\"{output_dir}/stock_indicators\"\n",
    "\n",
    "# Salvar como Parquet (formato otimizado)\n",
    "df_signals.coalesce(1).write.mode(\"overwrite\").parquet(indicators_path)\n",
    "print(f\"âœ… Dados salvos em: {indicators_path}\")\n",
    "\n",
    "# Salvar estatÃ­sticas de performance\n",
    "print(\"ðŸ“ˆ Salvando estatÃ­sticas de performance...\")\n",
    "performance_path = f\"{output_dir}/performance_stats\"\n",
    "performance_stats.coalesce(1).write.mode(\"overwrite\").parquet(performance_path)\n",
    "print(f\"âœ… EstatÃ­sticas salvas em: {performance_path}\")\n",
    "\n",
    "# Salvar dados mensais\n",
    "print(\"ðŸ“… Salvando dados de performance mensal...\")\n",
    "monthly_path = f\"{output_dir}/monthly_performance\"\n",
    "monthly_performance.coalesce(1).write.mode(\"overwrite\").parquet(monthly_path)\n",
    "print(f\"âœ… Dados mensais salvos em: {monthly_path}\")\n",
    "\n",
    "# Salvar como CSV tambÃ©m para compatibilidade\n",
    "print(\"ðŸ“„ Salvando versÃµes CSV...\")\n",
    "df_signals.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/stock_indicators_csv\")\n",
    "performance_stats.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/performance_stats_csv\")\n",
    "\n",
    "print(\"\\nâœ… Todos os dados processados foram salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ“Š VisualizaÃ§Ãµes dos Resultados\n",
    "\n",
    "Vamos criar algumas visualizaÃ§Ãµes dos dados processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter dados para visualizaÃ§Ã£o\n",
    "performance_pandas = performance_stats.toPandas()\n",
    "monthly_pandas = monthly_performance.toPandas()\n",
    "\n",
    "# GrÃ¡fico de performance mÃ©dia mensal\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "performance_pandas.set_index('symbol')['avg_monthly_return'].plot(kind='bar', color='skyblue')\n",
    "plt.title('ðŸ“ˆ Retorno MÃ©dio Mensal por Ativo', fontweight='bold')\n",
    "plt.ylabel('Retorno (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico de volatilidade\n",
    "plt.subplot(1, 2, 2)\n",
    "performance_pandas.set_index('symbol')['volatility'].plot(kind='bar', color='coral')\n",
    "plt.title('ðŸ“Š Volatilidade Mensal por Ativo', fontweight='bold')\n",
    "plt.ylabel('Volatilidade (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap de correlaÃ§Ãµes\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True, \n",
    "            linewidths=0.5,\n",
    "            fmt='.3f')\n",
    "plt.title('ðŸ”— Matriz de CorrelaÃ§Ã£o dos Retornos (Spark Processing)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. âš¡ OtimizaÃ§Ãµes de Performance\n",
    "\n",
    "Vamos demonstrar algumas otimizaÃ§Ãµes importantes do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lise de performance do Spark\n",
    "print(\"âš¡ AnÃ¡lise de Performance do Spark\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# InformaÃ§Ãµes sobre cache\n",
    "print(\"ðŸ’¾ DataFrames em Cache:\")\n",
    "cached_tables = spark.catalog.listTables()\n",
    "for table in cached_tables:\n",
    "    if table.isTemporary:\n",
    "        print(f\"  - {table.name}\")\n",
    "\n",
    "# EstatÃ­sticas do Spark Context\n",
    "sc = spark.sparkContext\n",
    "print(f\"\\nðŸ“Š EstatÃ­sticas do Spark Context:\")\n",
    "print(f\"  - Application ID: {sc.applicationId}\")\n",
    "print(f\"  - Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"  - Status: {sc.statusTracker().getExecutorInfos()}\")\n",
    "\n",
    "# Exemplo de particionamento otimizado\n",
    "print(\"\\nðŸ”§ OtimizaÃ§Ã£o de Particionamento:\")\n",
    "print(f\"PartiÃ§Ãµes atuais do DataFrame: {df_signals.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Reparticionamento por sÃ­mbolo para otimizar operaÃ§Ãµes por grupo\n",
    "df_optimized = df_signals.repartition(col(\"symbol\"))\n",
    "print(f\"PartiÃ§Ãµes apÃ³s reparticionamento: {df_optimized.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Exemplo de broadcast join (simulado)\n",
    "print(\"\\nðŸ“¡ Exemplo de Broadcast Join:\")\n",
    "# Criar pequena tabela de metadados\n",
    "metadata = spark.createDataFrame([\n",
    "    (\"AAPL\", \"Technology\", \"Apple Inc.\"),\n",
    "    (\"GOOGL\", \"Technology\", \"Alphabet Inc.\"),\n",
    "    (\"MSFT\", \"Technology\", \"Microsoft Corp.\"),\n",
    "    (\"TSLA\", \"Automotive\", \"Tesla Inc.\"),\n",
    "    (\"AMZN\", \"E-commerce\", \"Amazon.com Inc.\")\n",
    "], [\"symbol\", \"sector\", \"company_name\"])\n",
    "\n",
    "# Broadcast da tabela pequena\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_with_metadata = df_signals.join(\n",
    "    broadcast(metadata), \n",
    "    \"symbol\", \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Broadcast join configurado para otimizar performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ðŸ” Monitoramento e Debugging\n",
    "\n",
    "Vamos ver como monitorar jobs do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InformaÃ§Ãµes sobre jobs executados\n",
    "print(\"ðŸ” InformaÃ§Ãµes de Jobs do Spark\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Status tracker\n",
    "status_tracker = sc.statusTracker()\n",
    "\n",
    "# InformaÃ§Ãµes dos executors\n",
    "executor_infos = status_tracker.getExecutorInfos()\n",
    "print(f\"ðŸ“Š NÃºmero de Executors: {len(executor_infos)}\")\n",
    "\n",
    "for executor in executor_infos:\n",
    "    print(f\"\\nðŸ”§ Executor {executor.executorId}:\")\n",
    "    print(f\"  - Host: {executor.host}\")\n",
    "    print(f\"  - Cores: {executor.totalCores}\")\n",
    "    print(f\"  - MemÃ³ria MÃ¡xima: {executor.maxMemory / (1024**3):.2f} GB\")\n",
    "    print(f\"  - Tasks Ativas: {executor.activeTasks}\")\n",
    "    print(f\"  - Tasks Completadas: {executor.completedTasks}\")\n",
    "\n",
    "# URL da Spark UI\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "if spark_ui_url:\n",
    "    print(f\"\\nðŸŒ Spark UI disponÃ­vel em: {spark_ui_url}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Spark UI nÃ£o disponÃ­vel (modo local)\")\n",
    "\n",
    "# Exemplo de explain plan\n",
    "print(\"\\nðŸ“‹ Plano de ExecuÃ§Ã£o (Explain):\")\n",
    "print(\"=\" * 40)\n",
    "df_signals.filter(col(\"symbol\") == \"AAPL\").select(\"date\", \"close\", \"sma_20\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ðŸ§¹ Limpeza e FinalizaÃ§Ã£o\n",
    "\n",
    "Vamos limpar recursos e finalizar a sessÃ£o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar cache\n",
    "print(\"ðŸ§¹ Limpando cache...\")\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Unpersist DataFrames\n",
    "df.unpersist()\n",
    "df_indicators.unpersist()\n",
    "returns_pivot.unpersist()\n",
    "\n",
    "print(\"âœ… Cache limpo\")\n",
    "\n",
    "# EstatÃ­sticas finais\n",
    "print(\"\\nðŸ“Š RESUMO DO PROCESSAMENTO SPARK\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"ðŸ“ˆ Registros processados: {df.count():,}\")\n",
    "print(f\"ðŸ”§ Indicadores calculados: 8 (SMA, volatilidade, sinais, etc.)\")\n",
    "print(f\"ðŸ’¾ Arquivos salvos: 6 (Parquet + CSV)\")\n",
    "print(f\"âš¡ Executors utilizados: {len(executor_infos)}\")\n",
    "print(f\"ðŸŽ¯ Performance: Otimizada com cache e particionamento\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ PRÃ“XIMOS PASSOS:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"1. ðŸ“Š AnÃ¡lise estatÃ­stica avanÃ§ada\")\n",
    "print(\"2. ðŸ¤– Aplicar modelos de ML\")\n",
    "print(\"3. ðŸ’­ AnÃ¡lise de sentimentos\")\n",
    "print(\"4. ðŸ“ˆ Dashboards interativos\")\n",
    "print(\"5. ðŸ”„ Pipeline automatizado\")\n",
    "\n",
    "print(\"\\nâœ¨ Processamento Spark concluÃ­do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizar Spark Session\n",
    "print(\"ðŸ”š Finalizando Spark Session...\")\n",
    "spark.stop()\n",
    "print(\"âœ… Spark Session finalizada\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Notebook concluÃ­do com sucesso!\")\n",
    "print(\"ðŸ“ Dados processados disponÃ­veis em: ../data/processed/\")\n",
    "print(\"ðŸ”„ Execute o prÃ³ximo notebook para anÃ¡lises de ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š ReferÃªncias e Links Ãšteis\n",
    "\n",
    "- **Apache Spark**: [spark.apache.org](https://spark.apache.org/)\n",
    "- **PySpark Documentation**: [spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)\n",
    "- **Spark SQL Guide**: [spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- **Performance Tuning**: [spark.apache.org/docs/latest/tuning.html](https://spark.apache.org/docs/latest/tuning.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido pela Equipe Big Data Finance**  \n",
    "**Notebook:** 02_spark_processing_example.ipynb  \n",
    "**VersÃ£o:** 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}