{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Processamento com Apache Spark - Big Data Finance\n",
    "\n",
    "Este notebook demonstra como usar Apache Spark para processamento distribu√≠do de dados financeiros.\n",
    "\n",
    "## Objetivos\n",
    "- Configurar e inicializar Spark Session\n",
    "- Carregar dados financeiros no Spark\n",
    "- Realizar transforma√ß√µes distribu√≠das\n",
    "- Calcular indicadores t√©cnicos\n",
    "- Salvar resultados no HDFS\n",
    "- Demonstrar otimiza√ß√µes de performance\n",
    "\n",
    "**Autor:** Ana Luiza Pazze (Arquitetura e Infraestrutura) & Equipe Big Data Finance  \n",
    "**Gest√£o:** Fabio  \n",
    "**Processamento Spark:** Ana Luiza Pazze  \n",
    "**Data:** 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Imports do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Imports dos m√≥dulos do projeto\n",
    "from infrastructure.spark_manager import SparkManager\n",
    "from infrastructure.hdfs_manager import HDFSManager\n",
    "\n",
    "print(\"‚úÖ Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ‚öôÔ∏è Configura√ß√£o do Ambiente Spark\n",
    "\n",
    "Vamos configurar e inicializar o Spark para processamento distribu√≠do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Spark Manager\n",
    "spark_manager = SparkManager()\n",
    "\n",
    "# Criar Spark Session otimizada\n",
    "spark = spark_manager.create_spark_session(\n",
    "    app_name=\"BigDataFinance_Processing\",\n",
    "    executor_memory=\"2g\",\n",
    "    executor_cores=2,\n",
    "    num_executors=2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark Session criada com sucesso!\")\n",
    "print(f\"üîß Spark Version: {spark.version}\")\n",
    "print(f\"üîß Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"üîß Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Configura√ß√µes do Spark\n",
    "print(\"\\n‚öôÔ∏è Configura√ß√µes do Spark:\")\n",
    "print(\"=\" * 40)\n",
    "configs = spark.sparkContext.getConf().getAll()\n",
    "for key, value in sorted(configs):\n",
    "    if 'spark.sql' in key or 'spark.executor' in key or 'spark.driver' in key:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üìä Carregamento de Dados\n",
    "\n",
    "Vamos carregar os dados financeiros coletados anteriormente no Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se existem dados para carregar\n",
    "data_dir = '../data/raw'\n",
    "stock_files = [f for f in os.listdir(data_dir) if f.startswith('stock_data_') and f.endswith('.csv')]\n",
    "\n",
    "if stock_files:\n",
    "    # Usar o arquivo mais recente\n",
    "    latest_file = sorted(stock_files)[-1]\n",
    "    stock_file_path = os.path.join(data_dir, latest_file)\n",
    "    print(f\"üìÅ Carregando dados de: {latest_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum arquivo de dados encontrado. Execute primeiro o notebook 01_data_collection_example.ipynb\")\n",
    "    # Criar dados de exemplo para demonstra√ß√£o\n",
    "    print(\"üîß Criando dados de exemplo...\")\n",
    "    \n",
    "    # Gerar dados sint√©ticos\n",
    "    dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='D')\n",
    "    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "    \n",
    "    data = []\n",
    "    for symbol in symbols:\n",
    "        base_price = np.random.uniform(100, 300)\n",
    "        for date in dates:\n",
    "            price_change = np.random.normal(0, 0.02)\n",
    "            base_price *= (1 + price_change)\n",
    "            \n",
    "            data.append({\n",
    "                'date': date.strftime('%Y-%m-%d'),\n",
    "                'symbol': symbol,\n",
    "                'open': base_price * np.random.uniform(0.98, 1.02),\n",
    "                'high': base_price * np.random.uniform(1.00, 1.05),\n",
    "                'low': base_price * np.random.uniform(0.95, 1.00),\n",
    "                'close': base_price,\n",
    "                'volume': np.random.randint(1000000, 10000000)\n",
    "            })\n",
    "    \n",
    "    # Salvar dados sint√©ticos\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    stock_file_path = os.path.join(data_dir, 'stock_data_synthetic.csv')\n",
    "    pd.DataFrame(data).to_csv(stock_file_path, index=False)\n",
    "    print(f\"‚úÖ Dados sint√©ticos criados: {stock_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados no Spark\n",
    "print(\"üìä Carregando dados no Spark...\")\n",
    "\n",
    "# Definir schema para otimizar carregamento\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"volume\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Carregar DataFrame\n",
    "df = spark.read.csv(\n",
    "    stock_file_path,\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Converter coluna de data\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Cache para melhor performance\n",
    "df.cache()\n",
    "\n",
    "print(f\"‚úÖ Dados carregados: {df.count():,} registros\")\n",
    "print(f\"üìã Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Visualizar primeiros registros\n",
    "print(\"\\nüìä Primeiros registros:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîÑ Transforma√ß√µes B√°sicas\n",
    "\n",
    "Vamos realizar transforma√ß√µes b√°sicas nos dados usando Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar DataFrame como tabela tempor√°ria\n",
    "df.createOrReplaceTempView(\"stock_data\")\n",
    "\n",
    "# Estat√≠sticas b√°sicas por s√≠mbolo\n",
    "print(\"üìä Estat√≠sticas por S√≠mbolo:\")\n",
    "stats_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        COUNT(*) as records,\n",
    "        MIN(date) as start_date,\n",
    "        MAX(date) as end_date,\n",
    "        ROUND(AVG(close), 2) as avg_price,\n",
    "        ROUND(MIN(close), 2) as min_price,\n",
    "        ROUND(MAX(close), 2) as max_price,\n",
    "        ROUND(AVG(volume), 0) as avg_volume\n",
    "    FROM stock_data \n",
    "    GROUP BY symbol\n",
    "    ORDER BY symbol\n",
    "\"\"\")\n",
    "\n",
    "stats_df.show()\n",
    "\n",
    "# Converter para Pandas para visualiza√ß√£o\n",
    "stats_pandas = stats_df.toPandas()\n",
    "print(f\"\\nüìà Resumo: {len(stats_pandas)} s√≠mbolos analisados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üìà C√°lculo de Indicadores T√©cnicos\n",
    "\n",
    "Vamos calcular indicadores t√©cnicos usando Window Functions do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir window specifications\n",
    "window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "window_7d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "window_20d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-19, 0)\n",
    "window_50d = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-49, 0)\n",
    "\n",
    "print(\"üìä Calculando indicadores t√©cnicos...\")\n",
    "\n",
    "# Calcular indicadores\n",
    "df_indicators = df.withColumn(\n",
    "    \"daily_return\", \n",
    "    (col(\"close\") / lag(\"close\", 1).over(window_spec) - 1) * 100\n",
    ").withColumn(\n",
    "    \"sma_7\", \n",
    "    avg(\"close\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"sma_20\", \n",
    "    avg(\"close\").over(window_20d)\n",
    ").withColumn(\n",
    "    \"sma_50\", \n",
    "    avg(\"close\").over(window_50d)\n",
    ").withColumn(\n",
    "    \"volatility_7d\", \n",
    "    stddev(\"daily_return\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"price_change\", \n",
    "    col(\"close\") - col(\"open\")\n",
    ").withColumn(\n",
    "    \"price_range\", \n",
    "    col(\"high\") - col(\"low\")\n",
    ").withColumn(\n",
    "    \"volume_sma_20\", \n",
    "    avg(\"volume\").over(window_20d)\n",
    ")\n",
    "\n",
    "# Cache do resultado\n",
    "df_indicators.cache()\n",
    "\n",
    "print(\"‚úÖ Indicadores calculados!\")\n",
    "print(\"\\nüìä Exemplo de dados com indicadores:\")\n",
    "df_indicators.select(\n",
    "    \"date\", \"symbol\", \"close\", \"daily_return\", \n",
    "    \"sma_7\", \"sma_20\", \"volatility_7d\"\n",
    ").filter(\n",
    "    col(\"symbol\") == \"AAPL\"\n",
    ").orderBy(\n",
    "    desc(\"date\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 An√°lise de Sinais de Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar sinais de trading baseados em m√©dias m√≥veis\n",
    "print(\"üìà Gerando sinais de trading...\")\n",
    "\n",
    "df_signals = df_indicators.withColumn(\n",
    "    \"signal\",\n",
    "    when(col(\"sma_7\") > col(\"sma_20\"), \"BUY\")\n",
    "    .when(col(\"sma_7\") < col(\"sma_20\"), \"SELL\")\n",
    "    .otherwise(\"HOLD\")\n",
    ").withColumn(\n",
    "    \"trend\",\n",
    "    when(col(\"close\") > col(\"sma_50\"), \"UPTREND\")\n",
    "    .when(col(\"close\") < col(\"sma_50\"), \"DOWNTREND\")\n",
    "    .otherwise(\"SIDEWAYS\")\n",
    ").withColumn(\n",
    "    \"volatility_level\",\n",
    "    when(col(\"volatility_7d\") > 3.0, \"HIGH\")\n",
    "    .when(col(\"volatility_7d\") > 1.5, \"MEDIUM\")\n",
    "    .otherwise(\"LOW\")\n",
    ")\n",
    "\n",
    "# An√°lise de sinais por s√≠mbolo\n",
    "signals_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        signal,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY symbol), 2) as percentage\n",
    "    FROM (\n",
    "        SELECT symbol, \n",
    "               CASE \n",
    "                   WHEN sma_7 > sma_20 THEN 'BUY'\n",
    "                   WHEN sma_7 < sma_20 THEN 'SELL'\n",
    "                   ELSE 'HOLD'\n",
    "               END as signal\n",
    "        FROM stock_data_indicators\n",
    "        WHERE sma_7 IS NOT NULL AND sma_20 IS NOT NULL\n",
    "    )\n",
    "    GROUP BY symbol, signal\n",
    "    ORDER BY symbol, signal\n",
    "\"\"\")\n",
    "\n",
    "# Registrar nova tabela\n",
    "df_signals.createOrReplaceTempView(\"stock_data_indicators\")\n",
    "\n",
    "print(\"üìä Distribui√ß√£o de Sinais por S√≠mbolo:\")\n",
    "signals_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîç An√°lises Avan√ßadas com Spark SQL\n",
    "\n",
    "Vamos realizar an√°lises mais complexas usando Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de performance mensal\n",
    "print(\"üìÖ An√°lise de Performance Mensal:\")\n",
    "\n",
    "monthly_performance = spark.sql(\"\"\"\n",
    "    WITH monthly_data AS (\n",
    "        SELECT \n",
    "            symbol,\n",
    "            YEAR(date) as year,\n",
    "            MONTH(date) as month,\n",
    "            FIRST_VALUE(close) OVER (\n",
    "                PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                ORDER BY date\n",
    "            ) as month_open,\n",
    "            LAST_VALUE(close) OVER (\n",
    "                PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                ORDER BY date \n",
    "                ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "            ) as month_close,\n",
    "            AVG(volume) as avg_volume\n",
    "        FROM stock_data_indicators\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        symbol,\n",
    "        year,\n",
    "        month,\n",
    "        ROUND((month_close / month_open - 1) * 100, 2) as monthly_return,\n",
    "        ROUND(avg_volume, 0) as avg_volume\n",
    "    FROM monthly_data\n",
    "    ORDER BY symbol, year, month\n",
    "\"\"\")\n",
    "\n",
    "monthly_performance.show(20)\n",
    "\n",
    "# Estat√≠sticas de performance\n",
    "print(\"\\nüìä Estat√≠sticas de Performance:\")\n",
    "performance_stats = spark.sql(\"\"\"\n",
    "    WITH monthly_returns AS (\n",
    "        SELECT \n",
    "            symbol,\n",
    "            ROUND((month_close / month_open - 1) * 100, 2) as monthly_return\n",
    "        FROM (\n",
    "            SELECT DISTINCT\n",
    "                symbol,\n",
    "                YEAR(date) as year,\n",
    "                MONTH(date) as month,\n",
    "                FIRST_VALUE(close) OVER (\n",
    "                    PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                    ORDER BY date\n",
    "                ) as month_open,\n",
    "                LAST_VALUE(close) OVER (\n",
    "                    PARTITION BY symbol, YEAR(date), MONTH(date) \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "                ) as month_close\n",
    "            FROM stock_data_indicators\n",
    "        )\n",
    "    )\n",
    "    SELECT \n",
    "        symbol,\n",
    "        ROUND(AVG(monthly_return), 2) as avg_monthly_return,\n",
    "        ROUND(STDDEV(monthly_return), 2) as volatility,\n",
    "        ROUND(MIN(monthly_return), 2) as worst_month,\n",
    "        ROUND(MAX(monthly_return), 2) as best_month,\n",
    "        COUNT(*) as months_analyzed\n",
    "    FROM monthly_returns\n",
    "    GROUP BY symbol\n",
    "    ORDER BY avg_monthly_return DESC\n",
    "\"\"\")\n",
    "\n",
    "performance_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 An√°lise de Correla√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular correla√ß√µes entre ativos\n",
    "print(\"üîó An√°lise de Correla√ß√µes entre Ativos:\")\n",
    "\n",
    "# Pivot dos retornos di√°rios\n",
    "returns_pivot = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        MAX(CASE WHEN symbol = 'AAPL' THEN daily_return END) as AAPL,\n",
    "        MAX(CASE WHEN symbol = 'GOOGL' THEN daily_return END) as GOOGL,\n",
    "        MAX(CASE WHEN symbol = 'MSFT' THEN daily_return END) as MSFT,\n",
    "        MAX(CASE WHEN symbol = 'TSLA' THEN daily_return END) as TSLA,\n",
    "        MAX(CASE WHEN symbol = 'AMZN' THEN daily_return END) as AMZN\n",
    "    FROM stock_data_indicators\n",
    "    WHERE daily_return IS NOT NULL\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "returns_pivot.cache()\n",
    "print(\"üìä Matriz de retornos criada\")\n",
    "returns_pivot.show(10)\n",
    "\n",
    "# Converter para Pandas para calcular correla√ß√µes\n",
    "returns_pandas = returns_pivot.toPandas().set_index('date')\n",
    "correlation_matrix = returns_pandas.corr()\n",
    "\n",
    "print(\"\\nüîó Matriz de Correla√ß√£o:\")\n",
    "print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üíæ Salvamento no HDFS\n",
    "\n",
    "Vamos salvar os dados processados no HDFS para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar HDFS Manager (simulado para ambiente local)\n",
    "print(\"üíæ Preparando salvamento dos dados processados...\")\n",
    "\n",
    "# Criar diret√≥rio de sa√≠da\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Salvar dados com indicadores\n",
    "print(\"üìä Salvando dados com indicadores t√©cnicos...\")\n",
    "indicators_path = f\"{output_dir}/stock_indicators\"\n",
    "\n",
    "# Salvar como Parquet (formato otimizado)\n",
    "df_signals.coalesce(1).write.mode(\"overwrite\").parquet(indicators_path)\n",
    "print(f\"‚úÖ Dados salvos em: {indicators_path}\")\n",
    "\n",
    "# Salvar estat√≠sticas de performance\n",
    "print(\"üìà Salvando estat√≠sticas de performance...\")\n",
    "performance_path = f\"{output_dir}/performance_stats\"\n",
    "performance_stats.coalesce(1).write.mode(\"overwrite\").parquet(performance_path)\n",
    "print(f\"‚úÖ Estat√≠sticas salvas em: {performance_path}\")\n",
    "\n",
    "# Salvar dados mensais\n",
    "print(\"üìÖ Salvando dados de performance mensal...\")\n",
    "monthly_path = f\"{output_dir}/monthly_performance\"\n",
    "monthly_performance.coalesce(1).write.mode(\"overwrite\").parquet(monthly_path)\n",
    "print(f\"‚úÖ Dados mensais salvos em: {monthly_path}\")\n",
    "\n",
    "# Salvar como CSV tamb√©m para compatibilidade\n",
    "print(\"üìÑ Salvando vers√µes CSV...\")\n",
    "df_signals.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/stock_indicators_csv\")\n",
    "performance_stats.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/performance_stats_csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Todos os dados processados foram salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Visualiza√ß√µes dos Resultados\n",
    "\n",
    "Vamos criar algumas visualiza√ß√µes dos dados processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter dados para visualiza√ß√£o\n",
    "performance_pandas = performance_stats.toPandas()\n",
    "monthly_pandas = monthly_performance.toPandas()\n",
    "\n",
    "# Gr√°fico de performance m√©dia mensal\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "performance_pandas.set_index('symbol')['avg_monthly_return'].plot(kind='bar', color='skyblue')\n",
    "plt.title('üìà Retorno M√©dio Mensal por Ativo', fontweight='bold')\n",
    "plt.ylabel('Retorno (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico de volatilidade\n",
    "plt.subplot(1, 2, 2)\n",
    "performance_pandas.set_index('symbol')['volatility'].plot(kind='bar', color='coral')\n",
    "plt.title('üìä Volatilidade Mensal por Ativo', fontweight='bold')\n",
    "plt.ylabel('Volatilidade (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap de correla√ß√µes\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True, \n",
    "            linewidths=0.5,\n",
    "            fmt='.3f')\n",
    "plt.title('üîó Matriz de Correla√ß√£o dos Retornos (Spark Processing)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ‚ö° Otimiza√ß√µes de Performance\n",
    "\n",
    "Vamos demonstrar algumas otimiza√ß√µes importantes do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de performance do Spark\n",
    "print(\"‚ö° An√°lise de Performance do Spark\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Informa√ß√µes sobre cache\n",
    "print(\"üíæ DataFrames em Cache:\")\n",
    "cached_tables = spark.catalog.listTables()\n",
    "for table in cached_tables:\n",
    "    if table.isTemporary:\n",
    "        print(f\"  - {table.name}\")\n",
    "\n",
    "# Estat√≠sticas do Spark Context\n",
    "sc = spark.sparkContext\n",
    "print(f\"\\nüìä Estat√≠sticas do Spark Context:\")\n",
    "print(f\"  - Application ID: {sc.applicationId}\")\n",
    "print(f\"  - Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"  - Status: {sc.statusTracker().getExecutorInfos()}\")\n",
    "\n",
    "# Exemplo de particionamento otimizado\n",
    "print(\"\\nüîß Otimiza√ß√£o de Particionamento:\")\n",
    "print(f\"Parti√ß√µes atuais do DataFrame: {df_signals.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Reparticionamento por s√≠mbolo para otimizar opera√ß√µes por grupo\n",
    "df_optimized = df_signals.repartition(col(\"symbol\"))\n",
    "print(f\"Parti√ß√µes ap√≥s reparticionamento: {df_optimized.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Exemplo de broadcast join (simulado)\n",
    "print(\"\\nüì° Exemplo de Broadcast Join:\")\n",
    "# Criar pequena tabela de metadados\n",
    "metadata = spark.createDataFrame([\n",
    "    (\"AAPL\", \"Technology\", \"Apple Inc.\"),\n",
    "    (\"GOOGL\", \"Technology\", \"Alphabet Inc.\"),\n",
    "    (\"MSFT\", \"Technology\", \"Microsoft Corp.\"),\n",
    "    (\"TSLA\", \"Automotive\", \"Tesla Inc.\"),\n",
    "    (\"AMZN\", \"E-commerce\", \"Amazon.com Inc.\")\n",
    "], [\"symbol\", \"sector\", \"company_name\"])\n",
    "\n",
    "# Broadcast da tabela pequena\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_with_metadata = df_signals.join(\n",
    "    broadcast(metadata), \n",
    "    \"symbol\", \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Broadcast join configurado para otimizar performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üîç Monitoramento e Debugging\n",
    "\n",
    "Vamos ver como monitorar jobs do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes sobre jobs executados\n",
    "print(\"üîç Informa√ß√µes de Jobs do Spark\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Status tracker\n",
    "status_tracker = sc.statusTracker()\n",
    "\n",
    "# Informa√ß√µes dos executors\n",
    "executor_infos = status_tracker.getExecutorInfos()\n",
    "print(f\"üìä N√∫mero de Executors: {len(executor_infos)}\")\n",
    "\n",
    "for executor in executor_infos:\n",
    "    print(f\"\\nüîß Executor {executor.executorId}:\")\n",
    "    print(f\"  - Host: {executor.host}\")\n",
    "    print(f\"  - Cores: {executor.totalCores}\")\n",
    "    print(f\"  - Mem√≥ria M√°xima: {executor.maxMemory / (1024**3):.2f} GB\")\n",
    "    print(f\"  - Tasks Ativas: {executor.activeTasks}\")\n",
    "    print(f\"  - Tasks Completadas: {executor.completedTasks}\")\n",
    "\n",
    "# URL da Spark UI\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "if spark_ui_url:\n",
    "    print(f\"\\nüåê Spark UI dispon√≠vel em: {spark_ui_url}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Spark UI n√£o dispon√≠vel (modo local)\")\n",
    "\n",
    "# Exemplo de explain plan\n",
    "print(\"\\nüìã Plano de Execu√ß√£o (Explain):\")\n",
    "print(\"=\" * 40)\n",
    "df_signals.filter(col(\"symbol\") == \"AAPL\").select(\"date\", \"close\", \"sma_20\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üßπ Limpeza e Finaliza√ß√£o\n",
    "\n",
    "Vamos limpar recursos e finalizar a sess√£o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar cache\n",
    "print(\"üßπ Limpando cache...\")\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Unpersist DataFrames\n",
    "df.unpersist()\n",
    "df_indicators.unpersist()\n",
    "returns_pivot.unpersist()\n",
    "\n",
    "print(\"‚úÖ Cache limpo\")\n",
    "\n",
    "# Estat√≠sticas finais\n",
    "print(\"\\nüìä RESUMO DO PROCESSAMENTO SPARK\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"üìà Registros processados: {df.count():,}\")\n",
    "print(f\"üîß Indicadores calculados: 8 (SMA, volatilidade, sinais, etc.)\")\n",
    "print(f\"üíæ Arquivos salvos: 6 (Parquet + CSV)\")\n",
    "print(f\"‚ö° Executors utilizados: {len(executor_infos)}\")\n",
    "print(f\"üéØ Performance: Otimizada com cache e particionamento\")\n",
    "\n",
    "print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"1. üìä An√°lise estat√≠stica avan√ßada\")\n",
    "print(\"2. ü§ñ Aplicar modelos de ML\")\n",
    "print(\"3. üí≠ An√°lise de sentimentos\")\n",
    "print(\"4. üìà Dashboards interativos\")\n",
    "print(\"5. üîÑ Pipeline automatizado\")\n",
    "\n",
    "print(\"\\n‚ú® Processamento Spark conclu√≠do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizar Spark Session\n",
    "print(\"üîö Finalizando Spark Session...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark Session finalizada\")\n",
    "\n",
    "print(\"\\nüéâ Notebook conclu√≠do com sucesso!\")\n",
    "print(\"üìÅ Dados processados dispon√≠veis em: ../data/processed/\")\n",
    "print(\"üîÑ Execute o pr√≥ximo notebook para an√°lises de ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Refer√™ncias e Links √öteis\n",
    "\n",
    "- **Apache Spark**: [spark.apache.org](https://spark.apache.org/)\n",
    "- **PySpark Documentation**: [spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)\n",
    "- **Spark SQL Guide**: [spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- **Performance Tuning**: [spark.apache.org/docs/latest/tuning.html](https://spark.apache.org/docs/latest/tuning.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido pela Equipe Big Data Finance**  \n",
    "**Notebook:** 02_spark_processing_example.ipynb  \n",
    "**Vers√£o:** 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}